{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa8a775-2d8e-4c89-a8af-6ada7d39656c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is Good Practioce for the moment\n",
    "\n",
    "!rm -rf /opt/conda/lib/python3.10/site-packages/fsspec*\n",
    "!pip install fsspec==2024.6.0 --force-reinstall --no-deps\n",
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b54efa-6b1a-4818-a274-5c71090f7b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#install also to vizualize figures\n",
    "!sudo apt-get update\n",
    "!sudo apt-get install -y libgl1-mesa-glx\n",
    "!sudo apt-get install -y libglib2.0-0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "b9ecee34-ddef-4acf-ab8a-d0c7af3417fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the root directory for your Kaggle files\n",
    "rd = './kaggle-files'\n",
    "\n",
    "# Load the main CSV file\n",
    "df = pd.read_csv(f'{rd}/train.csv')\n",
    "df = df.fillna(-100)  # Use -100 to indicate missing labels\n",
    "\n",
    "# Map the labels to integers for multi-class classification\n",
    "label2id = {'Normal/Mild': 0, 'Moderate': 1, 'Severe': 2}\n",
    "df.replace(label2id, inplace=True)\n",
    "\n",
    "# Load the coordinates data\n",
    "coordinates_df = pd.read_csv(f'{rd}/dfc_updated.csv')\n",
    "# Keep only rows where 'slice_number' is not NaN\n",
    "coordinates_df = coordinates_df.dropna(subset=['slice_number'])\n",
    "coordinates_df['slice_number'] = coordinates_df['slice_number'].astype(int)\n",
    "\n",
    "# Load the series descriptions\n",
    "series_description_df = pd.read_csv(f'{rd}/train_series_descriptions.csv')\n",
    "series_description_df['series_description'] = series_description_df['series_description'].str.replace('T2/STIR', 'T2_STIR')\n",
    "\n",
    "# Define constants\n",
    "SERIES_DESCRIPTIONS = ['Sagittal T1', 'Sagittal T2_STIR', 'Axial T2']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "71d60a01-0b5a-4970-ad85-a3a505aec58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LumbarSpineDataset(Dataset):\n",
    "    def __init__(self, df, coordinates_df, series_description_df, root_dir, transform=None):\n",
    "        self.df = df\n",
    "        self.coordinates_df = coordinates_df\n",
    "        self.series_description_df = series_description_df\n",
    "        self.root_dir = root_dir  # The root directory where images are stored\n",
    "        self.transform = transform\n",
    "\n",
    "        # Get the list of study_ids\n",
    "        self.study_ids = self.df['study_id'].unique()\n",
    "\n",
    "        # List of label columns (assuming all columns except 'study_id' are labels)\n",
    "        self.label_columns = [col for col in df.columns if col != 'study_id']\n",
    "\n",
    "        # Prepare a mapping for images and annotations\n",
    "        self.study_image_paths = self._prepare_image_paths()\n",
    "\n",
    "        # Create a mapping from study_id to labels\n",
    "        self.labels_dict = self._prepare_labels()\n",
    "\n",
    "    def _prepare_image_paths(self):\n",
    "        study_image_paths = {}\n",
    "        for study_id in self.study_ids:\n",
    "            study_image_paths[study_id] = {}\n",
    "            for series_description in SERIES_DESCRIPTIONS:\n",
    "                series_description_clean = series_description.replace('/', '_')\n",
    "                image_dir = os.path.join(self.root_dir, 'cvt_png', str(study_id), series_description_clean)\n",
    "                if os.path.exists(image_dir):\n",
    "                    # Get all images in the directory\n",
    "                    image_paths = sorted(glob.glob(os.path.join(image_dir, '*.png')))\n",
    "                    study_image_paths[study_id][series_description] = image_paths\n",
    "                else:\n",
    "                    # Handle missing series\n",
    "                    study_image_paths[study_id][series_description] = []\n",
    "        return study_image_paths\n",
    "\n",
    "    def _prepare_labels(self):\n",
    "        labels_dict = {}\n",
    "        for idx, row in self.df.iterrows():\n",
    "            study_id = row['study_id']\n",
    "            labels = []\n",
    "            for col in self.label_columns:\n",
    "                label = row[col]\n",
    "                if pd.isnull(label) or label == -100:\n",
    "                    label = -100  # Use -100 for missing labels (ignore_index)\n",
    "                else:\n",
    "                    label = int(label)\n",
    "                labels.append(label)\n",
    "            labels_dict[study_id] = labels\n",
    "        return labels_dict\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.study_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        study_id = self.study_ids[idx]\n",
    "        images = {}\n",
    "        annotations = {}\n",
    "\n",
    "        # Load images for each series description\n",
    "        for series_description in SERIES_DESCRIPTIONS:\n",
    "            image_paths = self.study_image_paths[study_id][series_description]\n",
    "            series_images = []\n",
    "            for img_path in image_paths:\n",
    "                img = Image.open(img_path).convert('L')  # Convert to grayscale\n",
    "                if self.transform:\n",
    "                    img = self.transform(img)\n",
    "                series_images.append(img)\n",
    "            if series_images:\n",
    "                # Stack images along the depth dimension\n",
    "                series_tensor = torch.stack(series_images, dim=0)  # Shape: [num_slices, 1, H, W]\n",
    "            else:\n",
    "                # Handle missing images\n",
    "                series_tensor = torch.zeros((1, 1, 512, 512))  # Placeholder tensor\n",
    "            images[series_description] = series_tensor\n",
    "\n",
    "        # Get labels for the study_id\n",
    "        labels = self.labels_dict[study_id]\n",
    "        labels_tensor = torch.tensor(labels, dtype=torch.long)  # Use long dtype for CrossEntropyLoss\n",
    "\n",
    "        # Get annotations for the study_id (if needed)\n",
    "        study_annotations = self.coordinates_df[self.coordinates_df['study_id'] == study_id]\n",
    "        for _, row in study_annotations.iterrows():\n",
    "            condition = row['condition']\n",
    "            level = row['level']\n",
    "            x = row['x_scaled']\n",
    "            y = row['y_scaled']\n",
    "            series_description = row['series_description']\n",
    "            slice_number = int(row['slice_number'])\n",
    "            key = f\"{condition}_{level}\"\n",
    "            if key not in annotations:\n",
    "                annotations[key] = {}\n",
    "            if series_description not in annotations[key]:\n",
    "                annotations[key][series_description] = []\n",
    "            annotations[key][series_description].append({\n",
    "                'x': x,\n",
    "                'y': y,\n",
    "                'slice_number': slice_number\n",
    "            })\n",
    "\n",
    "        # Return a dictionary containing images, labels, and annotations\n",
    "        sample = {\n",
    "            'study_id': study_id,\n",
    "            'images': images,\n",
    "            'labels': labels_tensor,\n",
    "            'annotations': annotations\n",
    "        }\n",
    "\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "cefa0b40-fd4b-4763-b82c-a636b9899ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define any transformations if needed\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # Adjust mean and std if necessary\n",
    "])\n",
    "\n",
    "# Instantiate the dataset\n",
    "train_dataset = LumbarSpineDataset(\n",
    "    df=df,\n",
    "    coordinates_df=coordinates_df,\n",
    "    series_description_df=series_description_df,\n",
    "    root_dir='./rsna_output',  # Adjust the path as needed\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Create a DataLoader\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=1,  # Adjust batch size as needed\n",
    "    shuffle=True,\n",
    "    num_workers=4,  # Adjust based on your system\n",
    "    pin_memory=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "4c18c607-b316-4be8-9409-c07832e230b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ResNet feature extractor\n",
    "class ResNetFeatureExtractor(nn.Module):\n",
    "    def __init__(self, in_channels=10):\n",
    "        super(ResNetFeatureExtractor, self).__init__()\n",
    "        resnet = models.resnet18(pretrained=True)\n",
    "        # Modify the first convolutional layer to accept in_channels\n",
    "        resnet.conv1 = nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        # Extract layers up to layer4 (exclude avgpool and fc layers)\n",
    "        self.features = nn.Sequential(\n",
    "            resnet.conv1,\n",
    "            resnet.bn1,\n",
    "            resnet.relu,\n",
    "            resnet.maxpool,\n",
    "            resnet.layer1,\n",
    "            resnet.layer2,\n",
    "            resnet.layer3,\n",
    "            resnet.layer4\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return x  # Output shape: [batch_size, 512, H, W]\n",
    "\n",
    "# Define the main model\n",
    "class MultiSeriesSpineModel(nn.Module):\n",
    "    def __init__(self, num_conditions=25, num_classes=3):\n",
    "        super(MultiSeriesSpineModel, self).__init__()\n",
    "\n",
    "        # Feature extractors for each MRI series\n",
    "        self.cnn_sagittal_t1 = ResNetFeatureExtractor(in_channels=10)\n",
    "        self.cnn_sagittal_t2_stir = ResNetFeatureExtractor(in_channels=10)\n",
    "        self.cnn_axial_t2 = ResNetFeatureExtractor(in_channels=10)\n",
    "\n",
    "        # Define attention layers for each series\n",
    "        self.attention_sagittal_t1 = nn.Sequential(\n",
    "            nn.Conv2d(512, 1, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.attention_sagittal_t2_stir = nn.Sequential(\n",
    "            nn.Conv2d(512, 1, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.attention_axial_t2 = nn.Sequential(\n",
    "            nn.Conv2d(512, 1, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # Define the final classification layers\n",
    "        combined_feature_size = 512 * 3  # Since we're concatenating features from three models\n",
    "\n",
    "        self.fc1 = nn.Linear(combined_feature_size, 512)\n",
    "        self.fc2 = nn.Linear(512, num_conditions * num_classes)  # Output layer\n",
    "\n",
    "    def forward(self, sagittal_t1, sagittal_t2_stir, axial_t2):\n",
    "        # Forward pass through each ResNet18 model\n",
    "        features_sagittal_t1 = self.cnn_sagittal_t1(sagittal_t1)  # Shape: [batch_size, 512, H, W]\n",
    "        features_sagittal_t2_stir = self.cnn_sagittal_t2_stir(sagittal_t2_stir)\n",
    "        features_axial_t2 = self.cnn_axial_t2(axial_t2)\n",
    "\n",
    "        # Generate attention maps\n",
    "        attention_map_t1 = self.attention_sagittal_t1(features_sagittal_t1)  # Shape: [batch_size, 1, H, W]\n",
    "        attention_map_t2_stir = self.attention_sagittal_t2_stir(features_sagittal_t2_stir)\n",
    "        attention_map_axial = self.attention_axial_t2(features_axial_t2)\n",
    "\n",
    "        # Apply attention\n",
    "        features_sagittal_t1 = features_sagittal_t1 * attention_map_t1  # Element-wise multiplication\n",
    "        features_sagittal_t2_stir = features_sagittal_t2_stir * attention_map_t2_stir\n",
    "        features_axial_t2 = features_axial_t2 * attention_map_axial\n",
    "\n",
    "        # Global average pooling\n",
    "        features_sagittal_t1 = F.adaptive_avg_pool2d(features_sagittal_t1, (1, 1)).view(features_sagittal_t1.size(0), -1)\n",
    "        features_sagittal_t2_stir = F.adaptive_avg_pool2d(features_sagittal_t2_stir, (1, 1)).view(features_sagittal_t2_stir.size(0), -1)\n",
    "        features_axial_t2 = F.adaptive_avg_pool2d(features_axial_t2, (1, 1)).view(features_axial_t2.size(0), -1)\n",
    "\n",
    "        # Concatenate features\n",
    "        combined_features = torch.cat([features_sagittal_t1, features_sagittal_t2_stir, features_axial_t2], dim=1)\n",
    "\n",
    "        # Pass through final classification layers\n",
    "        x = F.relu(self.fc1(combined_features))\n",
    "        x = self.fc2(x)  # Shape: [batch_size, num_conditions * num_classes]\n",
    "        x = x.view(-1, num_conditions, num_classes)  # Reshape to [batch_size, num_conditions, num_classes]\n",
    "        return x  # Return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "26e3a38c-1ea4-44ec-be6a-6e2e1f4136ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample slices function\n",
    "def resample_slices(image_tensor, target_slices=10):\n",
    "    \"\"\"\n",
    "    Resample the number of slices to match the target number of slices.\n",
    "    \"\"\"\n",
    "    current_slices = image_tensor.shape[0]\n",
    "\n",
    "    if current_slices == target_slices:\n",
    "        return image_tensor  # No need to resample\n",
    "\n",
    "    # If more slices, downsample to the target number\n",
    "    if current_slices > target_slices:\n",
    "        indices = torch.linspace(0, current_slices - 1, target_slices).long()\n",
    "        return image_tensor[indices]\n",
    "\n",
    "    # If fewer slices, upsample by interpolation\n",
    "    image_tensor = image_tensor.permute(1, 0, 2, 3).unsqueeze(0)  # Shape: [1, channels, slices, H, W]\n",
    "    image_tensor_resized = F.interpolate(\n",
    "        image_tensor,\n",
    "        size=(target_slices, image_tensor.shape[3], image_tensor.shape[4]),\n",
    "        mode='trilinear',\n",
    "        align_corners=False\n",
    "    )\n",
    "    return image_tensor_resized.squeeze(0).permute(1, 0, 2, 3)  # Shape: [slices, channels, H, W]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "eedc0326-3bc8-45ee-b0b6-5e3aef8a7db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "num_conditions = len(train_dataset.label_columns)\n",
    "num_classes = 3\n",
    "model = MultiSeriesSpineModel(num_conditions=num_conditions, num_classes=num_classes)\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-100)  # Use ignore_index to ignore missing labels\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81676ba5-4374-4691-acd7-db15f793d89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:  11%|█         | 222/1975 [00:20<02:21, 12.38batch/s, Loss=0.6622]"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 10  # Define the number of epochs\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\")\n",
    "\n",
    "    for batch in progress_bar:\n",
    "        # Extract images and labels from the batch\n",
    "        images = batch['images']\n",
    "        labels = batch['labels']  # Tensor of shape [num_conditions]\n",
    "\n",
    "        # Get the image tensors\n",
    "        sagittal_t1 = images['Sagittal T1'].squeeze(0)  # Shape: [num_slices, 1, H, W]\n",
    "        sagittal_t2_stir = images['Sagittal T2_STIR'].squeeze(0)\n",
    "        axial_t2 = images['Axial T2'].squeeze(0)\n",
    "\n",
    "        # Resample slices to 10\n",
    "        sagittal_t1 = resample_slices(sagittal_t1, target_slices=10)\n",
    "        sagittal_t2_stir = resample_slices(sagittal_t2_stir, target_slices=10)\n",
    "        axial_t2 = resample_slices(axial_t2, target_slices=10)\n",
    "\n",
    "        # Remove singleton channel dimension if present\n",
    "        sagittal_t1 = sagittal_t1.squeeze(1)  # Shape: [10, H, W]\n",
    "        sagittal_t2_stir = sagittal_t2_stir.squeeze(1)\n",
    "        axial_t2 = axial_t2.squeeze(1)\n",
    "\n",
    "        # Add batch dimension and move to device\n",
    "        sagittal_t1 = sagittal_t1.unsqueeze(0).to(device)  # Shape: [1, 10, H, W]\n",
    "        sagittal_t2_stir = sagittal_t2_stir.unsqueeze(0).to(device)\n",
    "        axial_t2 = axial_t2.unsqueeze(0).to(device)\n",
    "\n",
    "        # Prepare labels tensor and move to device\n",
    "        labels_tensor = labels.unsqueeze(0).to(device)  # Shape: [1, num_conditions]\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(sagittal_t1, sagittal_t2_stir, axial_t2)  # Shape: [1, num_conditions, num_classes]\n",
    "\n",
    "        # Reshape outputs and labels\n",
    "        outputs = outputs.view(-1, num_classes)       # Shape: [num_conditions, num_classes]\n",
    "        labels_tensor = labels_tensor.view(-1)        # Shape: [num_conditions]\n",
    "\n",
    "        # Compute loss\n",
    "        total_loss = criterion(outputs, labels_tensor)\n",
    "\n",
    "        # Backward pass\n",
    "        total_loss.backward()\n",
    "\n",
    "        # Optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update loss\n",
    "        epoch_loss += total_loss.item()\n",
    "\n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({'Loss': f'{total_loss.item():.4f}'})\n",
    "\n",
    "    # Epoch summary\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] Average Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90573240-4869-40d9-b4fd-30ccc88801d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef34c129-3ffa-45ef-8edd-058ace6724bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee4db0c-7c47-426e-aa84-e53f43257251",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ed61b1-ca4d-4017-8304-5b060ba9ae15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c37e366-09d3-41d8-b1a7-141f31c5d950",
   "metadata": {},
   "source": [
    "\n",
    "### Objective:\n",
    "We want to plot an annotated axial slice image from our dataset. The annotations come from the `coordinates_df`, which contains x, y coordinates and additional information about the study, including the `series_id`, `instance_number`, `condition`, and `level`. These annotations represent the specific slices and the associated condition-level severity we're trying to classify/estimate.\n",
    "\n",
    "### Key Points:\n",
    "1. **Data Sources**:\n",
    "   - **`df`**: This contains the labels for `condition` and `level` across different spinal areas for each `study_id`.\n",
    "   - **`coordinates_df`**: This contains the x, y coordinates, `series_id`, `instance_number`, `condition`, and `level` related to each `study_id`.\n",
    "   - **`series_description_df`**: This maps the `series_id` to its respective `series_description` (e.g., 'Axial T2', 'Sagittal T1').\n",
    "\n",
    "2. **Image Path Mapping**:\n",
    "   - From `coordinates_df`, we need to extract the `study_id`, `series_id`, and `instance_number` to locate the corresponding axial image. \n",
    "   - The image path is generated using:\n",
    "     ```python\n",
    "     image_path = f'./rsna_output/cvt_png/{study_id}/{series_description}/{instance_number:03d}.png'\n",
    "     ```\n",
    "     where `series_description` is derived from the `series_id` using the `series_description_df`.\n",
    "\n",
    "3. **DataLoader Responsibilities**:\n",
    "   - The DataLoader needs to provide the required information (`study_id`, `series_id`, `instance_number`, `x`, `y`) to correctly map images and annotations.\n",
    "   - For slices without annotations, the model should focus on 'no annotation' data.\n",
    "\n",
    "### Process Flow:\n",
    "1. **Fetch Image and Annotations**:\n",
    "   - For each study (`study_id`), find the `x`, `y` coordinates from `coordinates_df`.\n",
    "   - Get the corresponding `series_id` and map it to a `series_description` using `series_description_df`.\n",
    "   - Locate the slice image using `series_description` and `instance_number`.\n",
    "\n",
    "2. **Plotting**:\n",
    "   - Display the axial slice image with a bounding box drawn around the `x`, `y` coordinates for the annotation.\n",
    "   - Display the label for the corresponding `condition` and `level`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a18821-1403-4841-98ea-0ab8d43b5dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchviz import make_dot\n",
    "# from PIL import Image\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Create dummy data to simulate model input\n",
    "# batch_size = 2\n",
    "# dummy_sagittal_t1 = torch.randn(batch_size, 10, 512, 512)  # 10 slices for Sagittal T1\n",
    "# dummy_sagittal_t2_stir = torch.randn(batch_size, 10, 512, 512)  # 10 slices for Sagittal T2/STIR\n",
    "# dummy_axial_t2 = torch.randn(batch_size, 10, 512, 512)  # 10 slices for Axial T2\n",
    "\n",
    "# # Pass through the model to get a forward pass\n",
    "# condition_pred, coord_pred = model(dummy_sagittal_t1, dummy_sagittal_t2_stir, dummy_axial_t2)\n",
    "\n",
    "# # Create the computational graph\n",
    "# dot = make_dot((condition_pred, coord_pred), params=dict(model.named_parameters()))\n",
    "\n",
    "# # Render to a file and display it\n",
    "# dot.render(\"model_diagram\", format=\"png\")  # Save as PNG\n",
    "\n",
    "# # Load and display the image\n",
    "# img = Image.open(\"model_diagram.png\")\n",
    "# plt.figure(figsize=(10, 10))  # Increase the figure size for better clarity\n",
    "# plt.imshow(img)\n",
    "# plt.axis('off')  # Hide axes for clarity\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
