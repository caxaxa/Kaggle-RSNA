{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ee5cf34c-7a7a-4268-8fdf-8593e838aba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from torchvision import transforms, models\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import KFold\n",
    "from torchvision.models import resnet18, ResNet18_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5d9dccfd-3e31-4d3c-a390-ed762a37f32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the root directory for your Kaggle files\n",
    "rd = './kaggle-files'\n",
    "\n",
    "# Load the main CSV file\n",
    "df = pd.read_csv(f'{rd}/train.csv')\n",
    "df = df.fillna(-100)  # Use -100 to indicate missing labels\n",
    "\n",
    "# Map the labels to integers for multi-class classification\n",
    "label2id = {'Normal/Mild': 0, 'Moderate': 1, 'Severe': 2}\n",
    "df.replace(label2id, inplace=True)\n",
    "\n",
    "# Load the coordinates data\n",
    "coordinates_df = pd.read_csv(f'{rd}/dfc_updated.csv')\n",
    "# Keep only rows where 'slice_number' is not NaN\n",
    "coordinates_df = coordinates_df.dropna(subset=['slice_number'])\n",
    "coordinates_df['slice_number'] = coordinates_df['slice_number'].astype(int)\n",
    "\n",
    "# Load the series descriptions\n",
    "series_description_df = pd.read_csv(f'{rd}/train_series_descriptions.csv')\n",
    "series_description_df['series_description'] = series_description_df['series_description'].str.replace('T2/STIR', 'T2_STIR')\n",
    "\n",
    "# Define constants\n",
    "SERIES_DESCRIPTIONS = ['Sagittal T1', 'Sagittal T2_STIR', 'Axial T2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "bfa71bad-ece2-4b94-9dda-48e1563b89e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LumbarSpineDataset(Dataset):\n",
    "    def __init__(self, df, coordinates_df, series_description_df, root_dir, transform=None):\n",
    "        self.df = df\n",
    "        self.coordinates_df = coordinates_df\n",
    "        self.series_description_df = series_description_df\n",
    "        self.root_dir = root_dir  # The root directory where images are stored\n",
    "        self.transform = transform\n",
    "\n",
    "        # Get the list of study_ids\n",
    "        self.study_ids = self.df['study_id'].unique()\n",
    "\n",
    "        # List of label columns (assuming all columns except 'study_id' are labels)\n",
    "        self.label_columns = [col for col in df.columns if col != 'study_id']\n",
    "\n",
    "        # Prepare a mapping for images and annotations\n",
    "        self.study_image_paths = self._prepare_image_paths()\n",
    "\n",
    "        # Create a mapping from study_id to labels\n",
    "        self.labels_dict = self._prepare_labels()\n",
    "\n",
    "    def _prepare_image_paths(self):\n",
    "        study_image_paths = {}\n",
    "        for study_id in self.study_ids:\n",
    "            study_image_paths[study_id] = {}\n",
    "            for series_description in SERIES_DESCRIPTIONS:\n",
    "                series_description_clean = series_description.replace('/', '_')\n",
    "                image_dir = os.path.join(self.root_dir, 'cvt_png', str(study_id), series_description_clean)\n",
    "                if os.path.exists(image_dir):\n",
    "                    # Get all images in the directory\n",
    "                    image_paths = sorted(glob.glob(os.path.join(image_dir, '*.png')))\n",
    "                    study_image_paths[study_id][series_description] = image_paths\n",
    "                else:\n",
    "                    # Handle missing series\n",
    "                    study_image_paths[study_id][series_description] = []\n",
    "        return study_image_paths\n",
    "\n",
    "    def _prepare_labels(self):\n",
    "        labels_dict = {}\n",
    "        for idx, row in self.df.iterrows():\n",
    "            study_id = row['study_id']\n",
    "            labels = []\n",
    "            for col in self.label_columns:\n",
    "                label = row[col]\n",
    "                if pd.isnull(label) or label == -100:\n",
    "                    label = -100  # Use -100 for missing labels (ignore_index)\n",
    "                else:\n",
    "                    label = int(label)\n",
    "                labels.append(label)\n",
    "            labels_dict[study_id] = labels\n",
    "        return labels_dict\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.study_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        study_id = self.study_ids[idx]\n",
    "        images = {}\n",
    "        annotations = {}\n",
    "\n",
    "        # Load images for each series description\n",
    "        for series_description in SERIES_DESCRIPTIONS:\n",
    "            image_paths = self.study_image_paths[study_id][series_description]\n",
    "            series_images = []\n",
    "            for img_path in image_paths:\n",
    "                img = Image.open(img_path).convert('L')  # Convert to grayscale\n",
    "                if self.transform:\n",
    "                    img = self.transform(img)  # Shape: [1, H, W]\n",
    "                    img = img.squeeze(0)  # Remove the channel dimension, resulting in [H, W]\n",
    "                series_images.append(img)\n",
    "            if series_images:\n",
    "                series_tensor = torch.stack(series_images, dim=0)  # Shape: [num_slices, H, W]\n",
    "            else:\n",
    "                series_tensor = torch.zeros((1, 512, 512))  # Placeholder tensor\n",
    "            images[series_description] = series_tensor  # Shape: [num_slices, H, W]\n",
    "\n",
    "        # Get labels for the study_id\n",
    "        labels = self.labels_dict[study_id]\n",
    "        labels_tensor = torch.tensor(labels, dtype=torch.long)  # Use long dtype for CrossEntropyLoss\n",
    "\n",
    "        # Generate attention masks, default to zeros if no annotations\n",
    "        attention_masks = {}\n",
    "        for series_description in SERIES_DESCRIPTIONS:\n",
    "            series_tensor = images[series_description]\n",
    "            num_slices = series_tensor.shape[0]\n",
    "            masks = []\n",
    "            for slice_idx in range(num_slices):\n",
    "                image_shape = series_tensor[slice_idx].shape  # Get (H, W)\n",
    "                mask = torch.zeros(image_shape, dtype=torch.float32)  # Default to zero mask\n",
    "                # If annotations exist, generate the attention mask\n",
    "                study_annotations = self.coordinates_df[self.coordinates_df['study_id'] == study_id]\n",
    "                for _, row in study_annotations.iterrows():\n",
    "                    if row['series_description'] == series_description:\n",
    "                        x_pixel = int(row['x_scaled'] * image_shape[1])\n",
    "                        y_pixel = int(row['y_scaled'] * image_shape[0])\n",
    "                        sigma = 5  # Adjust sigma\n",
    "                        y_grid, x_grid = torch.meshgrid(\n",
    "                            torch.arange(image_shape[0], dtype=torch.float32),\n",
    "                            torch.arange(image_shape[1], dtype=torch.float32),\n",
    "                            indexing='ij'\n",
    "                        )\n",
    "                        gauss = torch.exp(-((x_grid - x_pixel) ** 2 + (y_grid - y_pixel) ** 2) / (2 * sigma ** 2))\n",
    "                        mask = torch.maximum(mask, gauss)\n",
    "                masks.append(mask)\n",
    "            attention_masks[series_description] = torch.stack(masks, dim=0)  # Shape: [num_slices, H, W]\n",
    "\n",
    "        sample = {\n",
    "            'study_id': study_id,\n",
    "            'images': images,\n",
    "            'labels': labels_tensor,\n",
    "            'attention_masks': attention_masks\n",
    "        }\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "80b35b01-2a90-42e2-bb0f-027f01233078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define any transformations if needed\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # Adjust mean and std if necessary\n",
    "])\n",
    "\n",
    "# Instantiate the dataset\n",
    "train_dataset = LumbarSpineDataset(\n",
    "    df=df,\n",
    "    coordinates_df=coordinates_df,\n",
    "    series_description_df=series_description_df,\n",
    "    root_dir='./rsna_output',  # Adjust the path as needed\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Create a DataLoader\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=1,  # Adjust batch size as needed\n",
    "    shuffle=True,\n",
    "    num_workers=4,  # Adjust based on your system\n",
    "    pin_memory=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6b50a4b8-8839-4f50-98c6-55954e82bd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ResNet feature extractor\n",
    "class ResNetFeatureExtractor(nn.Module):\n",
    "    def __init__(self, in_channels=10):\n",
    "        super(ResNetFeatureExtractor, self).__init__()\n",
    "        # Load ResNet18 with the new weights argument\n",
    "        resnet = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "\n",
    "        # Modify the first convolutional layer to accept in_channels\n",
    "        resnet.conv1 = nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "        # Extract layers up to layer4 (exclude avgpool and fc layers)\n",
    "        self.features = nn.Sequential(\n",
    "            resnet.conv1,\n",
    "            resnet.bn1,\n",
    "            resnet.relu,\n",
    "            resnet.maxpool,\n",
    "            resnet.layer1,\n",
    "            resnet.layer2,\n",
    "            resnet.layer3,\n",
    "            resnet.layer4\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(f\"Input shape to ResNetFeatureExtractor: {x.shape}\")\n",
    "        x = self.features(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4bfbdb72-c0f7-444c-b000-6d27ea243ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the main model\n",
    "class MultiSeriesSpineModel(nn.Module):\n",
    "    def __init__(self, num_conditions=25, num_classes=3):\n",
    "        super(MultiSeriesSpineModel, self).__init__()\n",
    "        self.num_conditions = num_conditions\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Feature extractors for each MRI series\n",
    "        self.cnn_sagittal_t1 = ResNetFeatureExtractor(in_channels=10)\n",
    "        self.cnn_sagittal_t2_stir = ResNetFeatureExtractor(in_channels=10)\n",
    "        self.cnn_axial_t2 = ResNetFeatureExtractor(in_channels=10)\n",
    "\n",
    "        # Define attention layers for each series\n",
    "        self.attention_sagittal_t1 = nn.Sequential(\n",
    "            nn.Conv2d(512, 1, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.attention_sagittal_t2_stir = nn.Sequential(\n",
    "            nn.Conv2d(512, 1, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.attention_axial_t2 = nn.Sequential(\n",
    "            nn.Conv2d(512, 1, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # Define the final classification layers\n",
    "        combined_feature_size = 512 * 3  # Since we're concatenating features from three models\n",
    "\n",
    "        self.fc1 = nn.Linear(combined_feature_size, 512)\n",
    "        self.fc2 = nn.Linear(512, num_conditions * num_classes)  # Output layer\n",
    "\n",
    "    def forward(self, sagittal_t1, sagittal_t2_stir, axial_t2):\n",
    "        # The tensors are of shape [batch_size, in_channels, H, W]\n",
    "        #print(f\"sagittal_t1 shape before CNN: {sagittal_t1.shape}\")\n",
    "        #print(f\"sagittal_t2_stir shape before CNN: {sagittal_t2_stir.shape}\")\n",
    "        #print(f\"axial_t2 shape before CNN: {axial_t2.shape}\")\n",
    "\n",
    "        features_sagittal_t1 = self.cnn_sagittal_t1(sagittal_t1)  # Shape: [batch_size, 512, H, W]\n",
    "        features_sagittal_t2_stir = self.cnn_sagittal_t2_stir(sagittal_t2_stir)\n",
    "        features_axial_t2 = self.cnn_axial_t2(axial_t2)\n",
    "\n",
    "        # Generate attention maps (learned by the model)\n",
    "        attention_map_t1 = self.attention_sagittal_t1(features_sagittal_t1)  # Shape: [batch_size, 1, H, W]\n",
    "        attention_map_t2_stir = self.attention_sagittal_t2_stir(features_sagittal_t2_stir)\n",
    "        attention_map_axial = self.attention_axial_t2(features_axial_t2)\n",
    "\n",
    "        # Apply attention\n",
    "        attended_features_t1 = features_sagittal_t1 * attention_map_t1  # Element-wise multiplication\n",
    "        attended_features_t2_stir = features_sagittal_t2_stir * attention_map_t2_stir\n",
    "        attended_features_axial = features_axial_t2 * attention_map_axial\n",
    "\n",
    "        # Global average pooling\n",
    "        features_sagittal_t1 = F.adaptive_avg_pool2d(attended_features_t1, (1, 1)).view(attended_features_t1.size(0), -1)\n",
    "        features_sagittal_t2_stir = F.adaptive_avg_pool2d(attended_features_t2_stir, (1, 1)).view(attended_features_t2_stir.size(0), -1)\n",
    "        features_axial_t2 = F.adaptive_avg_pool2d(attended_features_axial, (1, 1)).view(attended_features_axial.size(0), -1)\n",
    "\n",
    "        # Concatenate features\n",
    "        combined_features = torch.cat([features_sagittal_t1, features_sagittal_t2_stir, features_axial_t2], dim=1)\n",
    "\n",
    "        # Pass through final classification layers\n",
    "        x = F.relu(self.fc1(combined_features))\n",
    "        x = self.fc2(x)  # Shape: [batch_size, num_conditions * num_classes]\n",
    "        x = x.view(-1, self.num_conditions, self.num_classes)  # Reshape to [batch_size, num_conditions, num_classes]\n",
    "\n",
    "        return x, [attention_map_t1, attention_map_t2_stir, attention_map_axial]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "25b9baaf-98b7-4f00-996c-e88d886466d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_slices(image_tensor, target_slices=10):\n",
    "    \"\"\"\n",
    "    Resample the number of slices to match the target number of slices.\n",
    "    Assumes image_tensor has shape [num_slices, H, W]\n",
    "    \"\"\"\n",
    "    #print(\"Shape of image_tensor in resample_slices:\", image_tensor.shape)\n",
    "    current_slices = image_tensor.shape[0]\n",
    "\n",
    "    if current_slices == target_slices:\n",
    "        return image_tensor  # No need to resample\n",
    "\n",
    "    # If more slices, downsample to the target number\n",
    "    if current_slices > target_slices:\n",
    "        indices = torch.linspace(0, current_slices - 1, target_slices).long()\n",
    "        return image_tensor[indices]\n",
    "\n",
    "    # If fewer slices, upsample by interpolation\n",
    "    # Add channel dimension\n",
    "    image_tensor = image_tensor.unsqueeze(0)  # Shape: [1, num_slices, H, W]\n",
    "    image_tensor = image_tensor.unsqueeze(0)  # Shape: [1, 1, num_slices, H, W]\n",
    "    image_tensor_resized = F.interpolate(\n",
    "        image_tensor,\n",
    "        size=(target_slices, image_tensor.shape[3], image_tensor.shape[4]),\n",
    "        mode='trilinear',\n",
    "        align_corners=False\n",
    "    )\n",
    "    image_tensor_resized = image_tensor_resized.squeeze(0).squeeze(0)  # Shape: [target_slices, H, W]\n",
    "    return image_tensor_resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "30cc1dcd-f863-4225-94e6-ba886180ea35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early Stopping class\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, delta=0):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.best_score = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "        self.best_loss = float('inf')\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Save model when validation loss decreases.'''\n",
    "        self.best_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'checkpoint.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "9078666e-013d-49a7-b87a-51a80219ac2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "num_conditions = len(train_dataset.label_columns)\n",
    "num_classes = 3\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Weight for the attention loss\n",
    "lambda_attention = 0.1  # Adjust the weighting factor as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "21bfd9e9-1a46-4dbe-8255-fbe0f93a130c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_k_fold(train_dataset, k_folds=5, num_epochs=10, model_save_path=False):\n",
    "    kfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "\n",
    "    for fold, (train_ids, val_ids) in enumerate(kfold.split(train_dataset)):\n",
    "        print(f'Fold {fold+1}/{k_folds}')\n",
    "\n",
    "        # Create data loaders for this fold\n",
    "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "        val_subsampler = torch.utils.data.SubsetRandomSampler(val_ids)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=1, sampler=train_subsampler, num_workers=4, pin_memory=True)\n",
    "        val_loader = DataLoader(train_dataset, batch_size=1, sampler=val_subsampler, num_workers=4, pin_memory=True)\n",
    "\n",
    "        # Initialize the model and move it to the correct device\n",
    "        model = MultiSeriesSpineModel(num_conditions=len(train_dataset.label_columns), num_classes=3)\n",
    "        model = model.to(device)\n",
    "\n",
    "        if model_save_path:\n",
    "            # Load the trained model's state_dict\n",
    "            model.load_state_dict(torch.load(model_save_path, map_location=device))\n",
    "\n",
    "        # Initialize optimizer and loss functions\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "        classification_criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "        attention_criterion = nn.MSELoss()\n",
    "\n",
    "        early_stopping = EarlyStopping(patience=3)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            total_train_loss = 0.0\n",
    "            total_val_loss = 0.0\n",
    "    \n",
    "            # Training loop\n",
    "            for batch in tqdm(train_loader, desc=f\"Fold {fold+1} Epoch {epoch+1}/{num_epochs}\", unit=\"batch\"):\n",
    "                # Extract images, labels, and attention masks\n",
    "                images = batch['images']\n",
    "                labels = batch['labels'].to(device)  # Move labels to the same device as model\n",
    "                attention_masks = batch['attention_masks']\n",
    "    \n",
    "                # Remove batch dimension before resampling\n",
    "                sagittal_t1 = resample_slices(images['Sagittal T1'].squeeze(0), target_slices=10).to(device)\n",
    "                sagittal_t2_stir = resample_slices(images['Sagittal T2_STIR'].squeeze(0), target_slices=10).to(device)\n",
    "                axial_t2 = resample_slices(images['Axial T2'].squeeze(0), target_slices=10).to(device)\n",
    "    \n",
    "                # Stack slices into the channel dimension\n",
    "                sagittal_t1 = sagittal_t1.unsqueeze(0)  # Add batch dimension: [1, num_slices, H, W]\n",
    "                sagittal_t1 = sagittal_t1.reshape(1, -1, 512, 512)  # Now, [batch_size, channels, H, W]\n",
    "    \n",
    "                sagittal_t2_stir = sagittal_t2_stir.unsqueeze(0).reshape(1, -1, 512, 512)\n",
    "                axial_t2 = axial_t2.unsqueeze(0).reshape(1, -1, 512, 512)\n",
    "\n",
    "                # Move attention masks to the same device\n",
    "                mask_t1 = attention_masks['Sagittal T1'].to(device)  # Shape: [batch_size, num_slices, H, W]\n",
    "                mask_t2_stir = attention_masks['Sagittal T2_STIR'].to(device)\n",
    "                mask_axial = attention_masks['Axial T2'].to(device)\n",
    "                \n",
    "                # Combine masks across slices (max over slices)\n",
    "                gt_mask_t1 = torch.max(mask_t1, dim=1)[0].unsqueeze(1)  # Shape: [batch_size, 1, H, W]\n",
    "                gt_mask_t2_stir = torch.max(mask_t2_stir, dim=1)[0].unsqueeze(1)\n",
    "                gt_mask_axial = torch.max(mask_axial, dim=1)[0].unsqueeze(1)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs, attention_maps = model(sagittal_t1, sagittal_t2_stir, axial_t2)\n",
    "                \n",
    "                # Reshape outputs and labels\n",
    "                outputs = outputs.view(-1, 3)\n",
    "                labels_tensor = labels.view(-1)\n",
    "                \n",
    "                # Compute classification loss\n",
    "                classification_loss = classification_criterion(outputs, labels_tensor)\n",
    "                \n",
    "                # Compute attention loss\n",
    "                attention_loss = 0.0\n",
    "                for att_map, gt_mask in zip(attention_maps, [gt_mask_t1, gt_mask_t2_stir, gt_mask_axial]):\n",
    "                    #print(\"Shape of att_map:\", att_map.shape)\n",
    "                    #print(\"Shape of gt_mask before interpolation:\", gt_mask.shape)\n",
    "                    # Resize ground truth mask to match attention map size\n",
    "                    gt_mask_resized = F.interpolate(\n",
    "                        gt_mask,\n",
    "                        size=att_map.shape[-2:],  # Match the size of the attention map\n",
    "                        mode='bilinear',\n",
    "                        align_corners=False\n",
    "                    )\n",
    "                    # Compute attention loss\n",
    "                    attention_loss += attention_criterion(att_map, gt_mask_resized)\n",
    "\n",
    "\n",
    "                # Total loss\n",
    "                total_loss = classification_loss + lambda_attention * attention_loss\n",
    "\n",
    "                # Zero gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Backpropagation and optimization\n",
    "                total_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_train_loss += total_loss.item()\n",
    "\n",
    "            avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "            print(f\"Fold {fold+1} Epoch [{epoch+1}/{num_epochs}] Avg Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "            # Early stopping\n",
    "            early_stopping(avg_train_loss, model)\n",
    "            if early_stopping.early_stop:\n",
    "                print(f\"Early stopping triggered for Fold {fold+1}!\")\n",
    "                break\n",
    "\n",
    "        # Load the last best checkpoint for this fold\n",
    "        model.load_state_dict(torch.load('checkpoint.pth'))\n",
    "        print(f\"Completed Fold {fold+1}/{k_folds}. Model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d7f763-ccba-465a-8b25-ca82683563cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 1/10: 100%|██████████| 1580/1580 [41:04<00:00,  1.56s/batch] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch [1/10] Avg Train Loss: 0.6742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 2/10: 100%|██████████| 1580/1580 [41:33<00:00,  1.58s/batch] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch [2/10] Avg Train Loss: 0.5741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 3/10: 100%|██████████| 1580/1580 [41:20<00:00,  1.57s/batch] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch [3/10] Avg Train Loss: 0.5270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 4/10: 100%|██████████| 1580/1580 [42:10<00:00,  1.60s/batch] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch [4/10] Avg Train Loss: 0.5001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 5/10:  85%|████████▍ | 1338/1580 [35:01<02:43,  1.48batch/s] "
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage\n",
    "train_k_fold(train_dataset, k_folds=5, num_epochs=10, model_save_path=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6d407b-d5f3-4e6d-848d-e7b4b6ae8778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model's state_dict\n",
    "model_save_path = 'spine_model_with_attention.pth'\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59131209-fefc-472e-bc7a-12f14611b529",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183253bb-abd8-4f02-9d1d-6699a84b8ee7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772345fa-a08b-4483-8783-8b601beb65e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f10b3b-a0f0-4cdb-b889-a49abdbffd75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b2bf37-d474-4e62-805b-e0a0950ad5e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bcca757-808c-4955-b8d1-5e285c6590c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the root directory for your Kaggle files\n",
    "rd = './kaggle-files'\n",
    "\n",
    "# Load the main CSV file\n",
    "df = pd.read_csv(f'{rd}/train.csv')\n",
    "df = df.fillna(-100)  # Use -100 to indicate missing labels\n",
    "\n",
    "# Map the labels to integers for multi-class classification\n",
    "label2id = {'Normal/Mild': 0, 'Moderate': 1, 'Severe': 2}\n",
    "df.replace(label2id, inplace=True)\n",
    "\n",
    "# Load the coordinates data\n",
    "coordinates_df = pd.read_csv(f'{rd}/dfc_updated.csv')\n",
    "# Keep only rows where 'slice_number' is not NaN\n",
    "coordinates_df = coordinates_df.dropna(subset=['slice_number'])\n",
    "coordinates_df['slice_number'] = coordinates_df['slice_number'].astype(int)\n",
    "\n",
    "# Load the series descriptions\n",
    "series_description_df = pd.read_csv(f'{rd}/train_series_descriptions.csv')\n",
    "series_description_df['series_description'] = series_description_df['series_description'].str.replace('T2/STIR', 'T2_STIR')\n",
    "\n",
    "# Define constants\n",
    "SERIES_DESCRIPTIONS = ['Sagittal T1', 'Sagittal T2_STIR', 'Axial T2']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59ffe72a-455e-4ae6-9726-66f840cf3fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['study_id', 'spinal_canal_stenosis_l1_l2',\n",
      "       'spinal_canal_stenosis_l2_l3', 'spinal_canal_stenosis_l3_l4',\n",
      "       'spinal_canal_stenosis_l4_l5', 'spinal_canal_stenosis_l5_s1',\n",
      "       'left_neural_foraminal_narrowing_l1_l2',\n",
      "       'left_neural_foraminal_narrowing_l2_l3',\n",
      "       'left_neural_foraminal_narrowing_l3_l4',\n",
      "       'left_neural_foraminal_narrowing_l4_l5',\n",
      "       'left_neural_foraminal_narrowing_l5_s1',\n",
      "       'right_neural_foraminal_narrowing_l1_l2',\n",
      "       'right_neural_foraminal_narrowing_l2_l3',\n",
      "       'right_neural_foraminal_narrowing_l3_l4',\n",
      "       'right_neural_foraminal_narrowing_l4_l5',\n",
      "       'right_neural_foraminal_narrowing_l5_s1',\n",
      "       'left_subarticular_stenosis_l1_l2', 'left_subarticular_stenosis_l2_l3',\n",
      "       'left_subarticular_stenosis_l3_l4', 'left_subarticular_stenosis_l4_l5',\n",
      "       'left_subarticular_stenosis_l5_s1', 'right_subarticular_stenosis_l1_l2',\n",
      "       'right_subarticular_stenosis_l2_l3',\n",
      "       'right_subarticular_stenosis_l3_l4',\n",
      "       'right_subarticular_stenosis_l4_l5',\n",
      "       'right_subarticular_stenosis_l5_s1'],\n",
      "      dtype='object')\n",
      "Index(['study_id', 'series_id', 'instance_number', 'condition', 'level', 'x',\n",
      "       'y', 'series_description', 'slice_number', 'original_height',\n",
      "       'original_width', 'x_scaled', 'y_scaled'],\n",
      "      dtype='object')\n",
      "Index(['study_id', 'series_id', 'series_description'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)\n",
    "print(coordinates_df.columns)\n",
    "print(series_description_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62ac1b78-c409-4670-9df1-4943b3175538",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LumbarSpineDataset(Dataset):\n",
    "    def __init__(self, df, coordinates_df, series_description_df, root_dir, transform=None):\n",
    "        self.df = df\n",
    "        self.coordinates_df = coordinates_df\n",
    "        self.series_description_df = series_description_df\n",
    "        self.root_dir = root_dir  # The root directory where images are stored\n",
    "        self.transform = transform\n",
    "\n",
    "        # Get the list of study_ids\n",
    "        self.study_ids = self.df['study_id'].unique()\n",
    "\n",
    "        # List of label columns (assuming all columns except 'study_id' are labels)\n",
    "        self.label_columns = [col for col in df.columns if col != 'study_id']\n",
    "\n",
    "        # Prepare a mapping for images and annotations\n",
    "        self.study_image_paths = self._prepare_image_paths()\n",
    "\n",
    "        # Create a mapping from study_id to labels\n",
    "        self.labels_dict = self._prepare_labels()\n",
    "\n",
    "    def _prepare_image_paths(self):\n",
    "        study_image_paths = {}\n",
    "        for study_id in self.study_ids:\n",
    "            study_image_paths[study_id] = {}\n",
    "            for series_description in SERIES_DESCRIPTIONS:\n",
    "                series_description_clean = series_description.replace('/', '_')\n",
    "                image_dir = os.path.join(self.root_dir, 'cvt_png', str(study_id), series_description_clean)\n",
    "                if os.path.exists(image_dir):\n",
    "                    # Get all images in the directory\n",
    "                    image_paths = sorted(glob.glob(os.path.join(image_dir, '*.png')))\n",
    "                    study_image_paths[study_id][series_description] = image_paths\n",
    "                else:\n",
    "                    # Handle missing series\n",
    "                    study_image_paths[study_id][series_description] = []\n",
    "        return study_image_paths\n",
    "\n",
    "    def _prepare_labels(self):\n",
    "        labels_dict = {}\n",
    "        for idx, row in self.df.iterrows():\n",
    "            study_id = row['study_id']\n",
    "            labels = []\n",
    "            for col in self.label_columns:\n",
    "                label = row[col]\n",
    "                if pd.isnull(label) or label == -100:\n",
    "                    label = -100  # Use -100 for missing labels (ignore_index)\n",
    "                else:\n",
    "                    label = int(label)\n",
    "                labels.append(label)\n",
    "            labels_dict[study_id] = labels\n",
    "        return labels_dict\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.study_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        study_id = self.study_ids[idx]\n",
    "        images = {}\n",
    "        annotations = {}\n",
    "\n",
    "        # Load images for each series description\n",
    "        for series_description in SERIES_DESCRIPTIONS:\n",
    "            image_paths = self.study_image_paths[study_id][series_description]\n",
    "            series_images = []\n",
    "            for img_path in image_paths:\n",
    "                img = Image.open(img_path).convert('L')  # Convert to grayscale\n",
    "                if self.transform:\n",
    "                    img = self.transform(img)\n",
    "                series_images.append(img)\n",
    "            if series_images:\n",
    "                series_tensor = torch.stack(series_images, dim=0)  # Shape: [num_slices, 1, H, W]\n",
    "            else:\n",
    "                series_tensor = torch.zeros((1, 1, 512, 512))  # Placeholder tensor\n",
    "            images[series_description] = series_tensor\n",
    "\n",
    "        # Get labels for the study_id\n",
    "        labels = self.labels_dict[study_id]\n",
    "        labels_tensor = torch.tensor(labels, dtype=torch.long)  # Use long dtype for CrossEntropyLoss\n",
    "\n",
    "        # Generate attention masks, default to zeros if no annotations\n",
    "        attention_masks = {}\n",
    "        for series_description in SERIES_DESCRIPTIONS:\n",
    "            series_tensor = images[series_description]\n",
    "            num_slices = series_tensor.shape[0]\n",
    "            masks = []\n",
    "            for slice_idx in range(num_slices):\n",
    "                image_shape = series_tensor[slice_idx].shape[-2:]  # Get (H, W)\n",
    "                mask = torch.zeros(image_shape, dtype=torch.float32)  # Default to zero mask\n",
    "                # If annotations exist, generate the attention mask\n",
    "                study_annotations = self.coordinates_df[self.coordinates_df['study_id'] == study_id]\n",
    "                for _, row in study_annotations.iterrows():\n",
    "                    if row['series_description'] == series_description and int(row['slice_number']) == slice_idx:\n",
    "                        x_pixel = int(row['x_scaled'] * image_shape[1])\n",
    "                        y_pixel = int(row['y_scaled'] * image_shape[0])\n",
    "                        sigma = 5  # Adjust sigma\n",
    "                        y_grid, x_grid = torch.meshgrid(\n",
    "                            torch.arange(image_shape[0], dtype=torch.float32),\n",
    "                            torch.arange(image_shape[1], dtype=torch.float32),\n",
    "                            indexing='ij'\n",
    "                        )\n",
    "                        gauss = torch.exp(-((x_grid - x_pixel) ** 2 + (y_grid - y_pixel) ** 2) / (2 * sigma ** 2))\n",
    "                        mask = torch.maximum(mask, gauss)\n",
    "                masks.append(mask)\n",
    "            attention_masks[series_description] = torch.stack(masks, dim=0) # Shape: [num_slices, 1, H, W]\n",
    "\n",
    "        sample = {\n",
    "            'study_id': study_id,\n",
    "            'images': images,\n",
    "            'labels': labels_tensor,\n",
    "            'attention_masks': attention_masks\n",
    "        }\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f32558a-a465-4e02-a3d6-cf7cbafad160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define any transformations if needed\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # Adjust mean and std if necessary\n",
    "])\n",
    "\n",
    "# Instantiate the dataset\n",
    "train_dataset = LumbarSpineDataset(\n",
    "    df=df,\n",
    "    coordinates_df=coordinates_df,\n",
    "    series_description_df=series_description_df,\n",
    "    root_dir='./rsna_output',  # Adjust the path as needed\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Create a DataLoader\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=1,  # Adjust batch size as needed\n",
    "    shuffle=True,\n",
    "    num_workers=4,  # Adjust based on your system\n",
    "    pin_memory=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5436dc84-08fe-499d-914d-2e07792e0f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ResNet feature extractor\n",
    "\n",
    "class ResNetFeatureExtractor(nn.Module):\n",
    "    def __init__(self, in_channels=10):\n",
    "        super(ResNetFeatureExtractor, self).__init__()\n",
    "        # Load ResNet18 with the new weights argument\n",
    "        resnet = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "\n",
    "        # Modify the first convolutional layer to accept in_channels\n",
    "        resnet.conv1 = nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "        # Extract layers up to layer4 (exclude avgpool and fc layers)\n",
    "        self.features = nn.Sequential(\n",
    "            resnet.conv1,\n",
    "            resnet.bn1,\n",
    "            resnet.relu,\n",
    "            resnet.maxpool,\n",
    "            resnet.layer1,\n",
    "            resnet.layer2,\n",
    "            resnet.layer3,\n",
    "            resnet.layer4\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f\"Input shape to ResNetFeatureExtractor: {x.shape}\")\n",
    "        x = self.features(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Define the main model\n",
    "class MultiSeriesSpineModel(nn.Module):\n",
    "    def __init__(self, num_conditions=25, num_classes=3):\n",
    "        super(MultiSeriesSpineModel, self).__init__()\n",
    "        self.num_conditions = num_conditions\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Feature extractors for each MRI series\n",
    "        self.cnn_sagittal_t1 = ResNetFeatureExtractor(in_channels=10)\n",
    "        self.cnn_sagittal_t2_stir = ResNetFeatureExtractor(in_channels=10)\n",
    "        self.cnn_axial_t2 = ResNetFeatureExtractor(in_channels=10)\n",
    "\n",
    "        # Define attention layers for each series\n",
    "        self.attention_sagittal_t1 = nn.Sequential(\n",
    "            nn.Conv2d(512, 1, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.attention_sagittal_t2_stir = nn.Sequential(\n",
    "            nn.Conv2d(512, 1, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.attention_axial_t2 = nn.Sequential(\n",
    "            nn.Conv2d(512, 1, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # Define the final classification layers\n",
    "        combined_feature_size = 512 * 3  # Since we're concatenating features from three models\n",
    "\n",
    "        self.fc1 = nn.Linear(combined_feature_size, 512)\n",
    "        self.fc2 = nn.Linear(512, num_conditions * num_classes)  # Output layer\n",
    "\n",
    "    def forward(self, sagittal_t1, sagittal_t2_stir, axial_t2):\n",
    "        # Forward pass through each ResNet18 model\n",
    "        \n",
    "        # Resample the slices if needed (assuming this has already been done)\n",
    "        \n",
    "        # Pool along the slice dimension to reduce [batch_size, slices, channels, H, W] to [batch_size, channels, H, W]\n",
    "        sagittal_t1 = torch.mean(sagittal_t1, dim=1)  # Use mean pooling (or max pooling) across the slice dimension\n",
    "        sagittal_t2_stir = torch.mean(sagittal_t2_stir, dim=1)\n",
    "        axial_t2 = torch.mean(axial_t2, dim=1)\n",
    "    \n",
    "        # Now the tensors are of shape [batch_size, channels, height, width]\n",
    "        print(f\"sagittal_t1 shape before CNN: {sagittal_t1.shape}\")\n",
    "        features_sagittal_t1 = self.cnn_sagittal_t1(sagittal_t1) # Shape: [batch_size, 512, H, W]\n",
    "        features_sagittal_t2_stir = self.cnn_sagittal_t2_stir(sagittal_t2_stir)\n",
    "        features_axial_t2 = self.cnn_axial_t2(axial_t2)\n",
    "    \n",
    "        # Generate attention maps (learned by the model)\n",
    "        attention_map_t1 = self.attention_sagittal_t1(features_sagittal_t1)  # Shape: [batch_size, 1, H, W]\n",
    "        attention_map_t2_stir = self.attention_sagittal_t2_stir(features_sagittal_t2_stir)\n",
    "        attention_map_axial = self.attention_axial_t2(features_axial_t2)\n",
    "    \n",
    "        # Apply attention\n",
    "        attended_features_t1 = features_sagittal_t1 * attention_map_t1  # Element-wise multiplication\n",
    "        attended_features_t2_stir = features_sagittal_t2_stir * attention_map_t2_stir\n",
    "        attended_features_axial = features_axial_t2 * attention_map_axial\n",
    "    \n",
    "        # Global average pooling\n",
    "        features_sagittal_t1 = F.adaptive_avg_pool2d(attended_features_t1, (1, 1)).view(attended_features_t1.size(0), -1)\n",
    "        features_sagittal_t2_stir = F.adaptive_avg_pool2d(attended_features_t2_stir, (1, 1)).view(attended_features_t2_stir.size(0), -1)\n",
    "        features_axial_t2 = F.adaptive_avg_pool2d(attended_features_axial, (1, 1)).view(attended_features_axial.size(0), -1)\n",
    "    \n",
    "        # Concatenate features\n",
    "        combined_features = torch.cat([features_sagittal_t1, features_sagittal_t2_stir, features_axial_t2], dim=1)\n",
    "    \n",
    "        # Pass through final classification layers\n",
    "        x = F.relu(self.fc1(combined_features))\n",
    "        x = self.fc2(x)  # Shape: [batch_size, num_conditions * num_classes]\n",
    "        x = x.view(-1, self.num_conditions, self.num_classes)  # Reshape to [batch_size, num_conditions, num_classes]\n",
    "    \n",
    "        return x, [attention_map_t1, attention_map_t2_stir, attention_map_axial]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "162f2684-489b-45f8-a76f-98b25d3c9164",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_slices(image_tensor, target_slices=10):\n",
    "    \"\"\"\n",
    "    Resample the number of slices to match the target number of slices.\n",
    "    \"\"\"\n",
    "    current_slices = image_tensor.shape[0]\n",
    "\n",
    "    if current_slices == target_slices:\n",
    "        return image_tensor  # No need to resample\n",
    "\n",
    "    # If more slices, downsample to the target number\n",
    "    if current_slices > target_slices:\n",
    "        indices = torch.linspace(0, current_slices - 1, target_slices).long()\n",
    "        return image_tensor[indices]\n",
    "\n",
    "    # If fewer slices, upsample by interpolation\n",
    "    image_tensor = image_tensor.permute(1, 0, 2, 3).unsqueeze(0)  # Shape: [1, channels, slices, H, W]\n",
    "    image_tensor_resized = F.interpolate(\n",
    "        image_tensor,\n",
    "        size=(target_slices, image_tensor.shape[3], image_tensor.shape[4]),\n",
    "        mode='trilinear',\n",
    "        align_corners=False\n",
    "    )\n",
    "    return image_tensor_resized.squeeze(0).permute(1, 0, 2, 3)  # Shape: [slices, channels, H, W]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "fc8fa435-fe21-4b82-9af6-6eee12d72740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early Stopping class\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, delta=0):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.best_score = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "        self.best_loss = float('inf')\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Save model when validation loss decreases.'''\n",
    "        self.best_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8af91c1f-2298-436a-97cc-6d5457f4c4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "num_conditions = len(train_dataset.label_columns)\n",
    "num_classes = 3\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Weight for the attention loss\n",
    "lambda_attention = 0.1  # Adjust the weighting factor as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3101c1dd-c997-4db6-937f-4bde5e3c602f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_k_fold(train_dataset, k_folds=5, num_epochs=10, model_save_path=False):\n",
    "    kfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "\n",
    "    for fold, (train_ids, val_ids) in enumerate(kfold.split(train_dataset)):\n",
    "        print(f'Fold {fold+1}/{k_folds}')\n",
    "\n",
    "        # Create data loaders for this fold\n",
    "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "        val_subsampler = torch.utils.data.SubsetRandomSampler(val_ids)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=1, sampler=train_subsampler, num_workers=4, pin_memory=True)\n",
    "        val_loader = DataLoader(train_dataset, batch_size=1, sampler=val_subsampler, num_workers=4, pin_memory=True)\n",
    "\n",
    "        # Initialize the model and move it to the correct device\n",
    "        model = MultiSeriesSpineModel(num_conditions=len(train_dataset.label_columns), num_classes=3)\n",
    "\n",
    "        # Always move the model to the correct device\n",
    "        model = model.to(device)\n",
    "\n",
    "        if model_save_path:\n",
    "            # Load the trained model's state_dict\n",
    "            model.load_state_dict(torch.load(model_save_path, map_location=device))\n",
    "\n",
    "        # Initialize optimizer and loss functions\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "        classification_criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "        attention_criterion = nn.MSELoss()\n",
    "\n",
    "        early_stopping = EarlyStopping(patience=3)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            total_train_loss = 0.0\n",
    "            total_val_loss = 0.0\n",
    "\n",
    "            # Training loop\n",
    "            for batch in tqdm(train_loader, desc=f\"Fold {fold+1} Epoch {epoch+1}/{num_epochs}\", unit=\"batch\"):\n",
    "                # Extract images, labels, and attention masks\n",
    "                images = batch['images']\n",
    "                labels = batch['labels'].to(device)  # Move labels to the same device as model\n",
    "                attention_masks = batch['attention_masks']\n",
    "\n",
    "                # Move all images to the same device\n",
    "                sagittal_t1 = resample_slices(images['Sagittal T1'].squeeze(0), target_slices=10).unsqueeze(0).to(device)\n",
    "                sagittal_t2_stir = resample_slices(images['Sagittal T2_STIR'].squeeze(0), target_slices=10).unsqueeze(0).to(device)\n",
    "                axial_t2 = resample_slices(images['Axial T2'].squeeze(0), target_slices=10).unsqueeze(0).to(device)\n",
    "\n",
    "                # Move attention masks to the same device\n",
    "                mask_t1 = attention_masks['Sagittal T1'].to(device)  # Shape: [num_slices, H, W]\n",
    "                mask_t2_stir = attention_masks['Sagittal T2_STIR'].to(device)\n",
    "                mask_axial = attention_masks['Axial T2'].to(device)\n",
    "                \n",
    "                # Combine masks across slices (max over slices)\n",
    "                gt_mask_t1 = torch.max(mask_t1, dim=0)[0].unsqueeze(0).unsqueeze(0)  # Shape: [1, 1, H, W]\n",
    "                gt_mask_t2_stir = torch.max(mask_t2_stir, dim=0)[0].unsqueeze(0).unsqueeze(0)\n",
    "                gt_mask_axial = torch.max(mask_axial, dim=0)[0].unsqueeze(0).unsqueeze(0)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs, attention_maps = model(sagittal_t1, sagittal_t2_stir, axial_t2)\n",
    "                \n",
    "                # Reshape outputs and labels\n",
    "                outputs = outputs.view(-1, 3)\n",
    "                labels_tensor = labels.view(-1)\n",
    "                \n",
    "                # Compute classification loss\n",
    "                classification_loss = classification_criterion(outputs, labels_tensor)\n",
    "                \n",
    "                # Compute attention loss\n",
    "                attention_loss = 0.0\n",
    "                for att_map, gt_mask in zip(attention_maps, [gt_mask_t1, gt_mask_t2_stir, gt_mask_axial]):\n",
    "                    print(\"Shape of att_map:\", att_map.shape)\n",
    "                    print(\"Shape of gt_mask before interpolation:\", gt_mask.shape)\n",
    "                    # Resize ground truth mask to match attention map size\n",
    "                    gt_mask_resized = F.interpolate(\n",
    "                        gt_mask,\n",
    "                        size=att_map.shape[-2:],  # Match the size of the attention map\n",
    "                        mode='bilinear',\n",
    "                        align_corners=False\n",
    "                    ).to(device)\n",
    "                \n",
    "                    # Compute attention loss\n",
    "                    attention_loss += attention_criterion(att_map, gt_mask_resized)\n",
    "\n",
    "\n",
    "                # Total loss\n",
    "                total_loss = classification_loss + 0.1 * attention_loss\n",
    "\n",
    "                # Zero gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Backpropagation and optimization\n",
    "                total_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_train_loss += total_loss.item()\n",
    "\n",
    "            # Validation loop\n",
    "\n",
    "            avg_train_loss = total_train_loss / len(train_loader)\n",
    "            avg_val_loss = total_val_loss / len(val_loader)\n",
    "\n",
    "            print(f\"Fold {fold+1} Epoch [{epoch+1}/{num_epochs}] Avg Train Loss: {avg_train_loss:.4f}, Avg Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "            # Early stopping\n",
    "            early_stopping(avg_val_loss, model)\n",
    "            if early_stopping.early_stop:\n",
    "                print(f\"Early stopping triggered for Fold {fold+1}!\")\n",
    "                break\n",
    "\n",
    "        # Load the last best checkpoint for this fold\n",
    "        model.load_state_dict(torch.load('checkpoint.pth'))\n",
    "        print(f\"Completed Fold {fold+1}/{k_folds}. Model saved.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "949df7cc-57f5-41c2-a3a5-d909178e2118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 1/10:   0%|          | 0/1580 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagittal_t1 shape before CNN: torch.Size([1, 1, 512, 512])\n",
      "Input shape to ResNetFeatureExtractor: torch.Size([1, 1, 512, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 1/10:   0%|          | 0/1580 [00:08<?, ?batch/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [64, 10, 7, 7], expected input[1, 1, 512, 512] to have 10 channels, but got 1 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrain_k_fold\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_folds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_save_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[83], line 59\u001b[0m, in \u001b[0;36mtrain_k_fold\u001b[0;34m(train_dataset, k_folds, num_epochs, model_save_path)\u001b[0m\n\u001b[1;32m     56\u001b[0m gt_mask_axial \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(mask_axial, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m outputs, attention_maps \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msagittal_t1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msagittal_t2_stir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxial_t2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Reshape outputs and labels\u001b[39;00m\n\u001b[1;32m     62\u001b[0m outputs \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[79], line 74\u001b[0m, in \u001b[0;36mMultiSeriesSpineModel.forward\u001b[0;34m(self, sagittal_t1, sagittal_t2_stir, axial_t2)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# Now the tensors are of shape [batch_size, channels, height, width]\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msagittal_t1 shape before CNN: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msagittal_t1\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 74\u001b[0m features_sagittal_t1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcnn_sagittal_t1\u001b[49m\u001b[43m(\u001b[49m\u001b[43msagittal_t1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Shape: [batch_size, 512, H, W]\u001b[39;00m\n\u001b[1;32m     75\u001b[0m features_sagittal_t2_stir \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcnn_sagittal_t2_stir(sagittal_t2_stir)\n\u001b[1;32m     76\u001b[0m features_axial_t2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcnn_axial_t2(axial_t2)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[79], line 26\u001b[0m, in \u001b[0;36mResNetFeatureExtractor.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput shape to ResNetFeatureExtractor: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 26\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 10, 7, 7], expected input[1, 1, 512, 512] to have 10 channels, but got 1 channels instead"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "train_k_fold(train_dataset, k_folds=5, num_epochs=10, model_save_path =  False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3b060f-21b0-49f1-bd82-bd9f6965ee6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3730a2-a57f-455a-9eee-85e68ba80a2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f343cf-bd13-4e2e-8375-d8b5b41f3dbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec737bd-bbcd-4448-a59a-bb6e2b165c34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "59bdb612-9d98-4aa5-b8de-b77dcd22469d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to multi_series_spine_model_w_attentio_v4.pth\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model's state_dict\n",
    "model_save_path = 'multi_series_spine_model_w_attentio_v4.pth'\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086c99b2-d64c-4c90-935c-9426adf8a369",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf69c4fd-970f-4fcc-a87f-592c1d273972",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5173840-6312-4a21-8578-35f7bb7c411a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b57eff-477a-4083-bdbf-e330d1d06ac8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed02d2d-f97a-429f-af86-2144c81d4968",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ffa0b6-ad21-48f0-ba15-baf464b1de2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7958d6a7-f38f-40a3-8836-d1f1f7fe0a6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a37e680-f7d6-4e3b-9f4c-ffdccdbee666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 10\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_classification_loss = 0.0\n",
    "    epoch_attention_loss = 0.0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\")\n",
    "\n",
    "    for batch_idx, batch in enumerate(progress_bar):\n",
    "        # Extract images, labels, and attention masks from the batch\n",
    "        images = batch['images']\n",
    "        labels = batch['labels']  # Tensor of shape [num_conditions]\n",
    "        attention_masks = batch['attention_masks']\n",
    "\n",
    "        # Get the image tensors\n",
    "        sagittal_t1 = images['Sagittal T1'].squeeze(0)  # Shape: [num_slices, 1, H, W]\n",
    "        sagittal_t2_stir = images['Sagittal T2_STIR'].squeeze(0)\n",
    "        axial_t2 = images['Axial T2'].squeeze(0)\n",
    "\n",
    "        # Resample slices to 10\n",
    "        sagittal_t1 = resample_slices(sagittal_t1, target_slices=10)\n",
    "        sagittal_t2_stir = resample_slices(sagittal_t2_stir, target_slices=10)\n",
    "        axial_t2 = resample_slices(axial_t2, target_slices=10)\n",
    "\n",
    "        # Remove singleton channel dimension if present\n",
    "        sagittal_t1 = sagittal_t1.squeeze(1)  # Shape: [10, H, W]\n",
    "        sagittal_t2_stir = sagittal_t2_stir.squeeze(1)\n",
    "        axial_t2 = axial_t2.squeeze(1)\n",
    "\n",
    "        # Add batch dimension and move to device\n",
    "        sagittal_t1 = sagittal_t1.unsqueeze(0).to(device)  # Shape: [1, 10, H, W]\n",
    "        sagittal_t2_stir = sagittal_t2_stir.unsqueeze(0).to(device)\n",
    "        axial_t2 = axial_t2.unsqueeze(0).to(device)\n",
    "\n",
    "        # Prepare labels tensor and move to device\n",
    "        labels_tensor = labels.unsqueeze(0).to(device)  # Shape: [1, num_conditions]\n",
    "\n",
    "        # Prepare attention masks and move to device\n",
    "        mask_t1 = attention_masks['Sagittal T1'].unsqueeze(0).to(device)       # Shape: [1, num_slices, 1, H, W]\n",
    "        mask_t2_stir = attention_masks['Sagittal T2_STIR'].unsqueeze(0).to(device)\n",
    "        mask_axial = attention_masks['Axial T2'].unsqueeze(0).to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs, attention_maps = model(sagittal_t1, sagittal_t2_stir, axial_t2)\n",
    "\n",
    "        # Reshape outputs and labels\n",
    "        outputs = outputs.view(-1, num_classes)\n",
    "        labels_tensor = labels_tensor.view(-1)\n",
    "\n",
    "        # Compute classification loss\n",
    "        classification_loss = classification_criterion(outputs, labels_tensor)\n",
    "\n",
    "        # Compute attention loss\n",
    "        attention_loss = 0.0\n",
    "        for att_map, gt_mask in zip(attention_maps, [mask_t1, mask_t2_stir, mask_axial]):\n",
    "            # Combine per-slice masks into a single mask\n",
    "            gt_mask_combined = torch.max(gt_mask, dim=1)[0]  # Shape: [batch_size, 1, 1, H, W]\n",
    "            gt_mask_combined = gt_mask_combined.squeeze(2)    # Now shape: [batch_size, 1, H, W]\n",
    "\n",
    "            # Resize ground truth mask to match attention map size\n",
    "            gt_mask_resized = F.interpolate(\n",
    "                gt_mask_combined,\n",
    "                size=att_map.shape[-2:],\n",
    "                mode='bilinear',\n",
    "                align_corners=False\n",
    "            )\n",
    "\n",
    "            # Compute attention loss\n",
    "            attention_loss += attention_criterion(att_map, gt_mask_resized)\n",
    "\n",
    "            # # Debugging: Print shapes on first iteration\n",
    "            # if epoch == 0 and batch_idx == 0:\n",
    "            #     print(f\"gt_mask shape: {gt_mask.shape}\")\n",
    "            #     print(f\"gt_mask_combined shape: {gt_mask_combined.shape}\")\n",
    "            #     print(f\"gt_mask_resized shape: {gt_mask_resized.shape}\")\n",
    "            #     print(f\"att_map shape: {att_map.shape}\")\n",
    "\n",
    "        # Total loss\n",
    "        total_loss = classification_loss + lambda_attention * attention_loss\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Backward pass\n",
    "        total_loss.backward()\n",
    "\n",
    "        # Optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update losses\n",
    "        epoch_classification_loss += classification_loss.item()\n",
    "        epoch_attention_loss += attention_loss.item()\n",
    "\n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\n",
    "            'Cls Loss': f'{classification_loss.item():.4f}',\n",
    "            'Att Loss': f'{attention_loss.item():.4f}'\n",
    "        })\n",
    "\n",
    "    # Epoch summary\n",
    "    avg_classification_loss = epoch_classification_loss / len(train_loader)\n",
    "    avg_attention_loss = epoch_attention_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] Avg Cls Loss: {avg_classification_loss:.4f}, Avg Att Loss: {avg_attention_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6869afb5-5ee5-4b78-b351-8c2d6e5cb5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model's state_dict\n",
    "model_save_path = 'multi_series_spine_model_w_attentio_v2.pth'\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4427fa2c-3c23-4913-9136-1fa16bb51e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    epoch_classification_loss = 0.0\n",
    "    epoch_attention_loss = 0.0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\")\n",
    "\n",
    "    for batch_idx, batch in enumerate(progress_bar):\n",
    "        # Extract images, labels, and attention masks from the batch\n",
    "        images = batch['images']\n",
    "        labels = batch['labels']  # Tensor of shape [num_conditions]\n",
    "        attention_masks = batch['attention_masks']\n",
    "\n",
    "        # Get the image tensors\n",
    "        sagittal_t1 = images['Sagittal T1'].squeeze(0)  # Shape: [num_slices, 1, H, W]\n",
    "        sagittal_t2_stir = images['Sagittal T2_STIR'].squeeze(0)\n",
    "        axial_t2 = images['Axial T2'].squeeze(0)\n",
    "\n",
    "        # Resample slices to 10\n",
    "        sagittal_t1 = resample_slices(sagittal_t1, target_slices=10)\n",
    "        sagittal_t2_stir = resample_slices(sagittal_t2_stir, target_slices=10)\n",
    "        axial_t2 = resample_slices(axial_t2, target_slices=10)\n",
    "\n",
    "        # Remove singleton channel dimension if present\n",
    "        sagittal_t1 = sagittal_t1.squeeze(1)  # Shape: [10, H, W]\n",
    "        sagittal_t2_stir = sagittal_t2_stir.squeeze(1)\n",
    "        axial_t2 = axial_t2.squeeze(1)\n",
    "\n",
    "        # Add batch dimension and move to device\n",
    "        sagittal_t1 = sagittal_t1.unsqueeze(0).to(device)  # Shape: [1, 10, H, W]\n",
    "        sagittal_t2_stir = sagittal_t2_stir.unsqueeze(0).to(device)\n",
    "        axial_t2 = axial_t2.unsqueeze(0).to(device)\n",
    "\n",
    "        # Prepare labels tensor and move to device\n",
    "        labels_tensor = labels.unsqueeze(0).to(device)  # Shape: [1, num_conditions]\n",
    "\n",
    "        # Prepare attention masks and move to device\n",
    "        mask_t1 = attention_masks['Sagittal T1'].unsqueeze(0).to(device)       # Shape: [1, num_slices, 1, H, W]\n",
    "        mask_t2_stir = attention_masks['Sagittal T2_STIR'].unsqueeze(0).to(device)\n",
    "        mask_axial = attention_masks['Axial T2'].unsqueeze(0).to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs, attention_maps = model(sagittal_t1, sagittal_t2_stir, axial_t2)\n",
    "\n",
    "        # Reshape outputs and labels\n",
    "        outputs = outputs.view(-1, num_classes)\n",
    "        labels_tensor = labels_tensor.view(-1)\n",
    "\n",
    "        # Compute classification loss\n",
    "        classification_loss = classification_criterion(outputs, labels_tensor)\n",
    "\n",
    "        # Compute attention loss\n",
    "        attention_loss = 0.0\n",
    "        for att_map, gt_mask in zip(attention_maps, [mask_t1, mask_t2_stir, mask_axial]):\n",
    "            # Combine per-slice masks into a single mask\n",
    "            gt_mask_combined = torch.max(gt_mask, dim=1)[0]  # Shape: [batch_size, 1, 1, H, W]\n",
    "            gt_mask_combined = gt_mask_combined.squeeze(2)    # Now shape: [batch_size, 1, H, W]\n",
    "\n",
    "            # Resize ground truth mask to match attention map size\n",
    "            gt_mask_resized = F.interpolate(\n",
    "                gt_mask_combined,\n",
    "                size=att_map.shape[-2:],\n",
    "                mode='bilinear',\n",
    "                align_corners=False\n",
    "            )\n",
    "\n",
    "            # Compute attention loss\n",
    "            attention_loss += attention_criterion(att_map, gt_mask_resized)\n",
    "\n",
    "            # # Debugging: Print shapes on first iteration\n",
    "            # if epoch == 0 and batch_idx == 0:\n",
    "            #     print(f\"gt_mask shape: {gt_mask.shape}\")\n",
    "            #     print(f\"gt_mask_combined shape: {gt_mask_combined.shape}\")\n",
    "            #     print(f\"gt_mask_resized shape: {gt_mask_resized.shape}\")\n",
    "            #     print(f\"att_map shape: {att_map.shape}\")\n",
    "\n",
    "        # Total loss\n",
    "        total_loss = classification_loss + lambda_attention * attention_loss\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Backward pass\n",
    "        total_loss.backward()\n",
    "\n",
    "        # Optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update losses\n",
    "        epoch_classification_loss += classification_loss.item()\n",
    "        epoch_attention_loss += attention_loss.item()\n",
    "\n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\n",
    "            'Cls Loss': f'{classification_loss.item():.4f}',\n",
    "            'Att Loss': f'{attention_loss.item():.4f}'\n",
    "        })\n",
    "\n",
    "    # Epoch summary\n",
    "    avg_classification_loss = epoch_classification_loss / len(train_loader)\n",
    "    avg_attention_loss = epoch_attention_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] Avg Cls Loss: {avg_classification_loss:.4f}, Avg Att Loss: {avg_attention_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7271a661-1abc-46f1-a763-ef40f9e1d787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model's state_dict\n",
    "model_save_path = 'multi_series_spine_model_w_attentio_40E.pth'\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a59a35-23aa-46d4-aa5e-30d6706aada1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    epoch_classification_loss = 0.0\n",
    "    epoch_attention_loss = 0.0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\")\n",
    "\n",
    "    for batch_idx, batch in enumerate(progress_bar):\n",
    "        # Extract images, labels, and attention masks from the batch\n",
    "        images = batch['images']\n",
    "        labels = batch['labels']  # Tensor of shape [num_conditions]\n",
    "        attention_masks = batch['attention_masks']\n",
    "\n",
    "        # Get the image tensors\n",
    "        sagittal_t1 = images['Sagittal T1'].squeeze(0)  # Shape: [num_slices, 1, H, W]\n",
    "        sagittal_t2_stir = images['Sagittal T2_STIR'].squeeze(0)\n",
    "        axial_t2 = images['Axial T2'].squeeze(0)\n",
    "\n",
    "        # Resample slices to 10\n",
    "        sagittal_t1 = resample_slices(sagittal_t1, target_slices=10)\n",
    "        sagittal_t2_stir = resample_slices(sagittal_t2_stir, target_slices=10)\n",
    "        axial_t2 = resample_slices(axial_t2, target_slices=10)\n",
    "\n",
    "        # Remove singleton channel dimension if present\n",
    "        sagittal_t1 = sagittal_t1.squeeze(1)  # Shape: [10, H, W]\n",
    "        sagittal_t2_stir = sagittal_t2_stir.squeeze(1)\n",
    "        axial_t2 = axial_t2.squeeze(1)\n",
    "\n",
    "        # Add batch dimension and move to device\n",
    "        sagittal_t1 = sagittal_t1.unsqueeze(0).to(device)  # Shape: [1, 10, H, W]\n",
    "        sagittal_t2_stir = sagittal_t2_stir.unsqueeze(0).to(device)\n",
    "        axial_t2 = axial_t2.unsqueeze(0).to(device)\n",
    "\n",
    "        # Prepare labels tensor and move to device\n",
    "        labels_tensor = labels.unsqueeze(0).to(device)  # Shape: [1, num_conditions]\n",
    "\n",
    "        # Prepare attention masks and move to device\n",
    "        mask_t1 = attention_masks['Sagittal T1'].unsqueeze(0).to(device)       # Shape: [1, num_slices, 1, H, W]\n",
    "        mask_t2_stir = attention_masks['Sagittal T2_STIR'].unsqueeze(0).to(device)\n",
    "        mask_axial = attention_masks['Axial T2'].unsqueeze(0).to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs, attention_maps = model(sagittal_t1, sagittal_t2_stir, axial_t2)\n",
    "\n",
    "        # Reshape outputs and labels\n",
    "        outputs = outputs.view(-1, num_classes)\n",
    "        labels_tensor = labels_tensor.view(-1)\n",
    "\n",
    "        # Compute classification loss\n",
    "        classification_loss = classification_criterion(outputs, labels_tensor)\n",
    "\n",
    "        # Compute attention loss\n",
    "        attention_loss = 0.0\n",
    "        for att_map, gt_mask in zip(attention_maps, [mask_t1, mask_t2_stir, mask_axial]):\n",
    "            # Combine per-slice masks into a single mask\n",
    "            gt_mask_combined = torch.max(gt_mask, dim=1)[0]  # Shape: [batch_size, 1, 1, H, W]\n",
    "            gt_mask_combined = gt_mask_combined.squeeze(2)    # Now shape: [batch_size, 1, H, W]\n",
    "\n",
    "            # Resize ground truth mask to match attention map size\n",
    "            gt_mask_resized = F.interpolate(\n",
    "                gt_mask_combined,\n",
    "                size=att_map.shape[-2:],\n",
    "                mode='bilinear',\n",
    "                align_corners=False\n",
    "            )\n",
    "\n",
    "            # Compute attention loss\n",
    "            attention_loss += attention_criterion(att_map, gt_mask_resized)\n",
    "\n",
    "            # # Debugging: Print shapes on first iteration\n",
    "            # if epoch == 0 and batch_idx == 0:\n",
    "            #     print(f\"gt_mask shape: {gt_mask.shape}\")\n",
    "            #     print(f\"gt_mask_combined shape: {gt_mask_combined.shape}\")\n",
    "            #     print(f\"gt_mask_resized shape: {gt_mask_resized.shape}\")\n",
    "            #     print(f\"att_map shape: {att_map.shape}\")\n",
    "\n",
    "        # Total loss\n",
    "        total_loss = classification_loss + lambda_attention * attention_loss\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Backward pass\n",
    "        total_loss.backward()\n",
    "\n",
    "        # Optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update losses\n",
    "        epoch_classification_loss += classification_loss.item()\n",
    "        epoch_attention_loss += attention_loss.item()\n",
    "\n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\n",
    "            'Cls Loss': f'{classification_loss.item():.4f}',\n",
    "            'Att Loss': f'{attention_loss.item():.4f}'\n",
    "        })\n",
    "\n",
    "    # Epoch summary\n",
    "    avg_classification_loss = epoch_classification_loss / len(train_loader)\n",
    "    avg_attention_loss = epoch_attention_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] Avg Cls Loss: {avg_classification_loss:.4f}, Avg Att Loss: {avg_attention_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc303397-ef52-4229-ab33-2d258a13ec1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model's state_dict\n",
    "model_save_path = 'multi_series_spine_model_w_attentio_50E.pth'\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a428aa8e-fd91-4977-8d6d-c35f8e4fb712",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    epoch_classification_loss = 0.0\n",
    "    epoch_attention_loss = 0.0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\")\n",
    "\n",
    "    for batch_idx, batch in enumerate(progress_bar):\n",
    "        # Extract images, labels, and attention masks from the batch\n",
    "        images = batch['images']\n",
    "        labels = batch['labels']  # Tensor of shape [num_conditions]\n",
    "        attention_masks = batch['attention_masks']\n",
    "\n",
    "        # Get the image tensors\n",
    "        sagittal_t1 = images['Sagittal T1'].squeeze(0)  # Shape: [num_slices, 1, H, W]\n",
    "        sagittal_t2_stir = images['Sagittal T2_STIR'].squeeze(0)\n",
    "        axial_t2 = images['Axial T2'].squeeze(0)\n",
    "\n",
    "        # Resample slices to 10\n",
    "        sagittal_t1 = resample_slices(sagittal_t1, target_slices=10)\n",
    "        sagittal_t2_stir = resample_slices(sagittal_t2_stir, target_slices=10)\n",
    "        axial_t2 = resample_slices(axial_t2, target_slices=10)\n",
    "\n",
    "        # Remove singleton channel dimension if present\n",
    "        sagittal_t1 = sagittal_t1.squeeze(1)  # Shape: [10, H, W]\n",
    "        sagittal_t2_stir = sagittal_t2_stir.squeeze(1)\n",
    "        axial_t2 = axial_t2.squeeze(1)\n",
    "\n",
    "        # Add batch dimension and move to device\n",
    "        sagittal_t1 = sagittal_t1.unsqueeze(0).to(device)  # Shape: [1, 10, H, W]\n",
    "        sagittal_t2_stir = sagittal_t2_stir.unsqueeze(0).to(device)\n",
    "        axial_t2 = axial_t2.unsqueeze(0).to(device)\n",
    "\n",
    "        # Prepare labels tensor and move to device\n",
    "        labels_tensor = labels.unsqueeze(0).to(device)  # Shape: [1, num_conditions]\n",
    "\n",
    "        # Prepare attention masks and move to device\n",
    "        mask_t1 = attention_masks['Sagittal T1'].unsqueeze(0).to(device)       # Shape: [1, num_slices, 1, H, W]\n",
    "        mask_t2_stir = attention_masks['Sagittal T2_STIR'].unsqueeze(0).to(device)\n",
    "        mask_axial = attention_masks['Axial T2'].unsqueeze(0).to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs, attention_maps = model(sagittal_t1, sagittal_t2_stir, axial_t2)\n",
    "\n",
    "        # Reshape outputs and labels\n",
    "        outputs = outputs.view(-1, num_classes)\n",
    "        labels_tensor = labels_tensor.view(-1)\n",
    "\n",
    "        # Compute classification loss\n",
    "        classification_loss = classification_criterion(outputs, labels_tensor)\n",
    "\n",
    "        # Compute attention loss\n",
    "        attention_loss = 0.0\n",
    "        for att_map, gt_mask in zip(attention_maps, [mask_t1, mask_t2_stir, mask_axial]):\n",
    "            # Combine per-slice masks into a single mask\n",
    "            gt_mask_combined = torch.max(gt_mask, dim=1)[0]  # Shape: [batch_size, 1, 1, H, W]\n",
    "            gt_mask_combined = gt_mask_combined.squeeze(2)    # Now shape: [batch_size, 1, H, W]\n",
    "\n",
    "            # Resize ground truth mask to match attention map size\n",
    "            gt_mask_resized = F.interpolate(\n",
    "                gt_mask_combined,\n",
    "                size=att_map.shape[-2:],\n",
    "                mode='bilinear',\n",
    "                align_corners=False\n",
    "            )\n",
    "\n",
    "            # Compute attention loss\n",
    "            attention_loss += attention_criterion(att_map, gt_mask_resized)\n",
    "\n",
    "            # # Debugging: Print shapes on first iteration\n",
    "            # if epoch == 0 and batch_idx == 0:\n",
    "            #     print(f\"gt_mask shape: {gt_mask.shape}\")\n",
    "            #     print(f\"gt_mask_combined shape: {gt_mask_combined.shape}\")\n",
    "            #     print(f\"gt_mask_resized shape: {gt_mask_resized.shape}\")\n",
    "            #     print(f\"att_map shape: {att_map.shape}\")\n",
    "\n",
    "        # Total loss\n",
    "        total_loss = classification_loss + lambda_attention * attention_loss\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Backward pass\n",
    "        total_loss.backward()\n",
    "\n",
    "        # Optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update losses\n",
    "        epoch_classification_loss += classification_loss.item()\n",
    "        epoch_attention_loss += attention_loss.item()\n",
    "\n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\n",
    "            'Cls Loss': f'{classification_loss.item():.4f}',\n",
    "            'Att Loss': f'{attention_loss.item():.4f}'\n",
    "        })\n",
    "\n",
    "    # Epoch summary\n",
    "    avg_classification_loss = epoch_classification_loss / len(train_loader)\n",
    "    avg_attention_loss = epoch_attention_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] Avg Cls Loss: {avg_classification_loss:.4f}, Avg Att Loss: {avg_attention_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0514df8-8669-4777-89d6-156907ad8bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model's state_dict\n",
    "model_save_path = 'multi_series_spine_model_w_attentio_60E.pth'\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae03994-9749-4cb1-994e-4ff9693616aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
