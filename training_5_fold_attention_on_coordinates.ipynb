{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f5bce19-3eaa-4221-a7d9-9aa696f67de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from torchvision import transforms, models\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import KFold\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba5a74c8-50ad-4a7a-abff-1d44e32fd706",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define constants\n",
    "SERIES_DESCRIPTIONS = ['Sagittal T1', 'Sagittal T2_STIR', 'Axial T2']\n",
    "CONDITIONS = [\n",
    "    'spinal_canal_stenosis', \n",
    "    'left_neural_foraminal_narrowing', \n",
    "    'right_neural_foraminal_narrowing',\n",
    "    'left_subarticular_stenosis',\n",
    "    'right_subarticular_stenosis'\n",
    "]\n",
    "LEVELS = [\n",
    "    'l1_l2',\n",
    "    'l2_l3',\n",
    "    'l3_l4',\n",
    "    'l4_l5',\n",
    "    'l5_s1',\n",
    "]\n",
    "LABELS = [f'{condition}_{level}' for condition in CONDITIONS for level in LEVELS]\n",
    "\n",
    "# Set the root directory for your Kaggle files\n",
    "rd = './kaggle-files'\n",
    "\n",
    "# Load the main CSV file\n",
    "df = pd.read_csv(f'{rd}/train.csv')\n",
    "\n",
    "df = df.fillna(-100)  # Use -100 to indicate missing labels\n",
    "\n",
    "# Map the labels to integers for multi-class classification\n",
    "label2id = {'Normal/Mild': 0, 'Moderate': 1, 'Severe': 2}\n",
    "df.replace(label2id, inplace=True)\n",
    "\n",
    "# Load the coordinates data\n",
    "coordinates_df = pd.read_csv(f'{rd}/dfc_updated.csv')\n",
    "# Keep only rows where 'slice_number' is not NaN\n",
    "coordinates_df = coordinates_df.dropna(subset=['slice_number'])\n",
    "coordinates_df['slice_number'] = coordinates_df['slice_number'].astype(int)\n",
    "\n",
    "# Load the series descriptions\n",
    "series_description_df = pd.read_csv(f'{rd}/train_series_descriptions.csv')\n",
    "series_description_df['series_description'] = series_description_df['series_description'].str.replace('T2/STIR', 'T2_STIR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b50b818-381c-4d63-9e96-5f8cfcd02c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class LumbarSpineDataset(Dataset):\n",
    "    def __init__(self, df, coordinates_df, series_description_df, root_dir, transform=None):\n",
    "        self.df = df\n",
    "        self.coordinates_df = coordinates_df\n",
    "        self.series_description_df = series_description_df\n",
    "        self.root_dir = root_dir  # The root directory where images are stored\n",
    "        self.transform = transform\n",
    "\n",
    "        # Get the list of study_ids\n",
    "        self.study_ids = self.df['study_id'].unique()\n",
    "\n",
    "        # List of label columns\n",
    "        self.label_columns = [col for col in df.columns if col != 'study_id']\n",
    "\n",
    "        # Prepare a mapping for images and annotations\n",
    "        self.study_image_paths = self._prepare_image_paths()\n",
    "\n",
    "        # Create a mapping from study_id to labels\n",
    "        self.labels_dict = self._prepare_labels()\n",
    "\n",
    "    def _prepare_image_paths(self):\n",
    "        study_image_paths = {}\n",
    "        for study_id in self.study_ids:\n",
    "            study_image_paths[study_id] = {}\n",
    "            for series_description in SERIES_DESCRIPTIONS:\n",
    "                series_description_clean = series_description.replace('/', '_')\n",
    "                image_dir = os.path.join(self.root_dir, 'cvt_png', str(study_id), series_description_clean)\n",
    "                if os.path.exists(image_dir):\n",
    "                    # Get all images in the directory\n",
    "                    image_paths = sorted(glob.glob(os.path.join(image_dir, '*.png')))\n",
    "                    study_image_paths[study_id][series_description] = image_paths\n",
    "                else:\n",
    "                    # Handle missing series\n",
    "                    study_image_paths[study_id][series_description] = []\n",
    "        return study_image_paths\n",
    "\n",
    "    def _prepare_labels(self):\n",
    "        labels_dict = {}\n",
    "        for idx, row in self.df.iterrows():\n",
    "            study_id = row['study_id']\n",
    "            labels = []\n",
    "            for col in self.label_columns:\n",
    "                label = row[col]\n",
    "                if pd.isnull(label) or label == -100:\n",
    "                    label = -100  # Use -100 for missing labels (ignore_index)\n",
    "                else:\n",
    "                    label = int(label)\n",
    "                labels.append(label)\n",
    "            labels_dict[study_id] = labels\n",
    "        return labels_dict\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.study_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        study_id = self.study_ids[idx]\n",
    "        images = {}\n",
    "        annotations = {}\n",
    "\n",
    "        # Load images for each series description\n",
    "        for series_description in SERIES_DESCRIPTIONS:\n",
    "            image_paths = self.study_image_paths[study_id][series_description]\n",
    "            series_images = []\n",
    "            for img_path in image_paths:\n",
    "                img = Image.open(img_path).convert('L')  # Convert to grayscale\n",
    "                if self.transform:\n",
    "                    img = self.transform(img)  # Shape: [1, H, W]\n",
    "                    img = img.squeeze(0)  # Remove the channel dimension, resulting in [H, W]\n",
    "                series_images.append(img)\n",
    "            if series_images:\n",
    "                series_tensor = torch.stack(series_images, dim=0)  # Shape: [num_slices, H, W]\n",
    "            else:\n",
    "                series_tensor = torch.zeros((1, 512, 512))  # Placeholder tensor\n",
    "            images[series_description] = series_tensor  # Shape: [num_slices, H, W]\n",
    "\n",
    "        # Get labels for the study_id\n",
    "        labels = self.labels_dict[study_id]\n",
    "        labels_tensor = torch.tensor(labels, dtype=torch.long)  # Use long dtype for CrossEntropyLoss\n",
    "\n",
    "        # Generate attention masks, default to zeros if no annotations\n",
    "        attention_masks = {}\n",
    "        for series_description in SERIES_DESCRIPTIONS:\n",
    "            series_tensor = images[series_description]\n",
    "            num_slices = series_tensor.shape[0]\n",
    "            masks = []\n",
    "            for slice_idx in range(num_slices):\n",
    "                image_shape = series_tensor[slice_idx].shape  # Get (H, W)\n",
    "                mask = torch.zeros(image_shape, dtype=torch.float32)  # Default to zero mask\n",
    "                # If annotations exist, generate the attention mask\n",
    "                study_annotations = self.coordinates_df[self.coordinates_df['study_id'] == study_id]\n",
    "                for _, row in study_annotations.iterrows():\n",
    "                    if row['series_description'] == series_description:\n",
    "                        x_pixel = int(row['x_scaled'] * image_shape[1])\n",
    "                        y_pixel = int(row['y_scaled'] * image_shape[0])\n",
    "                        sigma = 5  # Adjust sigma\n",
    "                        y_grid, x_grid = torch.meshgrid(\n",
    "                            torch.arange(image_shape[0], dtype=torch.float32),\n",
    "                            torch.arange(image_shape[1], dtype=torch.float32),\n",
    "                            indexing='ij'\n",
    "                        )\n",
    "                        gauss = torch.exp(-((x_grid - x_pixel) ** 2 + (y_grid - y_pixel) ** 2) / (2 * sigma ** 2))\n",
    "                        mask = torch.maximum(mask, gauss)\n",
    "                masks.append(mask)\n",
    "            attention_masks[series_description] = torch.stack(masks, dim=0)  # Shape: [num_slices, H, W]\n",
    "\n",
    "        sample = {\n",
    "            'study_id': study_id,\n",
    "            'images': images,\n",
    "            'labels': labels_tensor,\n",
    "            'attention_masks': attention_masks\n",
    "        }\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4855d643-ca6a-47d3-bfc1-449459363c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define any transformations if needed\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # Adjust mean and std if necessary\n",
    "])\n",
    "\n",
    "# Instantiate the dataset\n",
    "train_dataset = LumbarSpineDataset(\n",
    "    df=df,\n",
    "    coordinates_df=coordinates_df,\n",
    "    series_description_df=series_description_df,\n",
    "    root_dir='./rsna_output',  # Adjust the path as needed\n",
    "    transform=transform\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae09e7b2-843d-44fd-8689-6539fcb967d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_slices(image_tensor, target_slices=10):\n",
    "    # Ensure the image tensor has at least 3 dimensions\n",
    "    if image_tensor.dim() == 2:\n",
    "        image_tensor = image_tensor.unsqueeze(0)  # Add slice dimension\n",
    "    current_slices = image_tensor.shape[0]\n",
    "    if current_slices == target_slices:\n",
    "        return image_tensor  # No need to resample\n",
    "    if current_slices > target_slices:\n",
    "        indices = torch.linspace(0, current_slices - 1, target_slices).long()\n",
    "        return image_tensor[indices]\n",
    "    # If fewer slices, upsample\n",
    "    image_tensor = image_tensor.unsqueeze(0).unsqueeze(0)  # Shape: [1, 1, num_slices, H, W]\n",
    "    image_tensor_resized = F.interpolate(\n",
    "        image_tensor,\n",
    "        size=(target_slices, image_tensor.shape[3], image_tensor.shape[4]),\n",
    "        mode='trilinear',\n",
    "        align_corners=False\n",
    "    )\n",
    "    image_tensor_resized = image_tensor_resized.squeeze(0).squeeze(0)  # Shape: [target_slices, H, W]\n",
    "    return image_tensor_resized\n",
    "\n",
    "# Early Stopping class\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, delta=0):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.best_score = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "        self.best_loss = float('inf')\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Save model when validation loss decreases.'''\n",
    "        self.best_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'checkpoint.pth')\n",
    "\n",
    "# Define the ResNet feature extractor\n",
    "class ResNetFeatureExtractor(nn.Module):\n",
    "    def __init__(self, in_channels=10):\n",
    "        super(ResNetFeatureExtractor, self).__init__()\n",
    "        resnet = models.resnet18(pretrained=True)\n",
    "\n",
    "        # Modify the first convolutional layer to accept in_channels\n",
    "        resnet.conv1 = nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "        # Extract layers up to layer4 (exclude avgpool and fc layers)\n",
    "        self.features = nn.Sequential(\n",
    "            resnet.conv1,\n",
    "            resnet.bn1,\n",
    "            resnet.relu,\n",
    "            resnet.maxpool,\n",
    "            resnet.layer1,\n",
    "            resnet.layer2,\n",
    "            resnet.layer3,\n",
    "            resnet.layer4\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return x\n",
    "\n",
    "# Define the main model\n",
    "class MultiSeriesSpineModel(nn.Module):\n",
    "    def __init__(self, num_conditions=25, num_classes=3):\n",
    "        super(MultiSeriesSpineModel, self).__init__()\n",
    "        self.num_conditions = num_conditions\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Feature extractors for each MRI series\n",
    "        self.cnn_sagittal_t1 = ResNetFeatureExtractor(in_channels=10)\n",
    "        self.cnn_sagittal_t2_stir = ResNetFeatureExtractor(in_channels=10)\n",
    "        self.cnn_axial_t2 = ResNetFeatureExtractor(in_channels=10)\n",
    "\n",
    "        # Define attention layers for each series\n",
    "        self.attention_sagittal_t1 = nn.Sequential(\n",
    "            nn.Conv2d(512, 1, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.attention_sagittal_t2_stir = nn.Sequential(\n",
    "            nn.Conv2d(512, 1, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.attention_axial_t2 = nn.Sequential(\n",
    "            nn.Conv2d(512, 1, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # Define the final classification layers\n",
    "        combined_feature_size = 512 * 3  # Since we're concatenating features from three models\n",
    "\n",
    "        self.fc1 = nn.Linear(combined_feature_size, 512)\n",
    "        self.fc2 = nn.Linear(512, num_conditions * num_classes)  # Output layer\n",
    "\n",
    "    def forward(self, sagittal_t1, sagittal_t2_stir, axial_t2):\n",
    "        # The tensors are of shape [batch_size, in_channels, H, W]\n",
    "        features_sagittal_t1 = self.cnn_sagittal_t1(sagittal_t1)  # Shape: [batch_size, 512, H, W]\n",
    "        features_sagittal_t2_stir = self.cnn_sagittal_t2_stir(sagittal_t2_stir)\n",
    "        features_axial_t2 = self.cnn_axial_t2(axial_t2)\n",
    "\n",
    "        # Generate attention maps (learned by the model)\n",
    "        attention_map_t1 = self.attention_sagittal_t1(features_sagittal_t1)  # Shape: [batch_size, 1, H, W]\n",
    "        attention_map_t2_stir = self.attention_sagittal_t2_stir(features_sagittal_t2_stir)\n",
    "        attention_map_axial = self.attention_axial_t2(features_axial_t2)\n",
    "\n",
    "        # Apply attention\n",
    "        attended_features_t1 = features_sagittal_t1 * attention_map_t1  # Element-wise multiplication\n",
    "        attended_features_t2_stir = features_sagittal_t2_stir * attention_map_t2_stir\n",
    "        attended_features_axial = features_axial_t2 * attention_map_axial\n",
    "\n",
    "        # Global average pooling\n",
    "        features_sagittal_t1 = F.adaptive_avg_pool2d(attended_features_t1, (1, 1)).view(attended_features_t1.size(0), -1)\n",
    "        features_sagittal_t2_stir = F.adaptive_avg_pool2d(attended_features_t2_stir, (1, 1)).view(attended_features_t2_stir.size(0), -1)\n",
    "        features_axial_t2 = F.adaptive_avg_pool2d(attended_features_axial, (1, 1)).view(attended_features_axial.size(0), -1)\n",
    "\n",
    "        # Concatenate features\n",
    "        combined_features = torch.cat([features_sagittal_t1, features_sagittal_t2_stir, features_axial_t2], dim=1)\n",
    "\n",
    "        # Pass through final classification layers\n",
    "        x = F.relu(self.fc1(combined_features))\n",
    "        x = self.fc2(x)  # Shape: [batch_size, num_conditions * num_classes]\n",
    "        x = x.view(-1, self.num_conditions, self.num_classes)  # Reshape to [batch_size, num_conditions, num_classes]\n",
    "\n",
    "        return x, [attention_map_t1, attention_map_t2_stir, attention_map_axial]\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def custom_loss(outputs, labels, any_severe_scalar=0.5):\n",
    "    batch_size = outputs.shape[0]\n",
    "    num_conditions = outputs.shape[1]\n",
    "    num_classes = outputs.shape[2]\n",
    "\n",
    "    # Map conditions to label indices\n",
    "    condition_to_indices = {}\n",
    "    for condition in CONDITIONS:\n",
    "        condition_to_indices[condition] = [i for i, label in enumerate(LABELS) if condition in label]\n",
    "\n",
    "    condition_losses = []\n",
    "    condition_weights = []\n",
    "\n",
    "    classification_criterion = nn.CrossEntropyLoss(ignore_index=-100, reduction='mean')\n",
    "\n",
    "    for condition in CONDITIONS:\n",
    "        indices = condition_to_indices[condition]\n",
    "        outputs_condition = outputs[:, indices, :]  # Shape: [batch_size, num_labels_in_condition, num_classes]\n",
    "        labels_condition = labels[:, indices]  # Shape: [batch_size, num_labels_in_condition]\n",
    "\n",
    "        # Reshape to compute loss\n",
    "        outputs_condition = outputs_condition.reshape(-1, num_classes)\n",
    "        labels_condition = labels_condition.reshape(-1)\n",
    "\n",
    "        # Exclude samples with ignore_index\n",
    "        valid_mask = labels_condition != -100\n",
    "        if valid_mask.sum() > 0:\n",
    "            outputs_valid = outputs_condition[valid_mask]\n",
    "            labels_valid = labels_condition[valid_mask]\n",
    "\n",
    "            loss_condition = classification_criterion(outputs_valid, labels_valid)\n",
    "            condition_losses.append(loss_condition)\n",
    "            condition_weights.append(1.0)\n",
    "        else:\n",
    "            pass  # Skip if all labels are missing\n",
    "\n",
    "    # Compute 'any severe' loss for 'spinal_canal_stenosis'\n",
    "    spinal_indices = condition_to_indices['spinal_canal_stenosis']\n",
    "    outputs_spinal = outputs[:, spinal_indices, :]  # Shape: [batch_size, num_spinal_labels, num_classes]\n",
    "    labels_spinal = labels[:, spinal_indices]  # Shape: [batch_size, num_spinal_labels]\n",
    "\n",
    "    # Get severe class outputs (class index 2)\n",
    "    severe_preds_spinal = outputs_spinal[:, :, 2]  # Shape: [batch_size, num_spinal_labels]\n",
    "    severe_labels_spinal = (labels_spinal == 2).float()  # Shape: [batch_size, num_spinal_labels]\n",
    "\n",
    "    # Ignore labels with -100\n",
    "    valid_mask = labels_spinal != -100\n",
    "    severe_preds_spinal = severe_preds_spinal * valid_mask.float()\n",
    "    severe_labels_spinal = severe_labels_spinal * valid_mask.float()\n",
    "\n",
    "    # For each sample in batch, get max severe label and prediction\n",
    "    any_severe_label = torch.max(severe_labels_spinal, dim=1)[0]  # Shape: [batch_size]\n",
    "    any_severe_pred = torch.max(severe_preds_spinal, dim=1)[0]  # Shape: [batch_size]\n",
    "\n",
    "    # Compute binary cross-entropy loss\n",
    "    any_severe_loss = F.binary_cross_entropy_with_logits(any_severe_pred, any_severe_label)\n",
    "\n",
    "    # Append to losses\n",
    "    condition_losses.append(any_severe_loss)\n",
    "    condition_weights.append(any_severe_scalar)\n",
    "\n",
    "    # Compute weighted average of losses\n",
    "    total_loss = 0.0\n",
    "    total_weight = sum(condition_weights)\n",
    "    for loss, weight in zip(condition_losses, condition_weights):\n",
    "        total_loss += loss * weight\n",
    "    total_loss = total_loss / total_weight\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42b30baa-9e89-4605-95b4-e48a117d9993",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def custom_collate_fn(batch):\n",
    "    collated_batch = {}\n",
    "    # Handle 'study_id' separately\n",
    "    collated_batch['study_id'] = [item['study_id'] for item in batch]\n",
    "    # Handle 'labels'\n",
    "    labels_list = []\n",
    "    for item in batch:\n",
    "        labels = item['labels']\n",
    "        if not isinstance(labels, torch.Tensor):\n",
    "            labels = torch.tensor(labels, dtype=torch.long)\n",
    "        if labels.dim() == 0:\n",
    "            labels = labels.unsqueeze(0)\n",
    "        labels_list.append(labels)\n",
    "    collated_batch['labels'] = torch.stack(labels_list)\n",
    "    # Handle 'images' and 'attention_masks'\n",
    "    for key in ['images', 'attention_masks']:\n",
    "        collated_batch[key] = {}\n",
    "        sub_keys = batch[0][key].keys()\n",
    "        for sub_key in sub_keys:\n",
    "            items_list = []\n",
    "            for item in batch:\n",
    "                data = item[key][sub_key]\n",
    "                if not isinstance(data, torch.Tensor):\n",
    "                    data = torch.tensor(data)\n",
    "                items_list.append(data)\n",
    "            collated_batch[key][sub_key] = torch.stack(items_list)\n",
    "    return collated_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31c948de-693d-4d5b-9be8-177639c6c1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusted training loop with try-except blocks\n",
    "def train_k_fold_with_custom_loss(train_dataset, k_folds=5, num_epochs=10, any_severe_scalar=0.5, lambda_attention=0.1):\n",
    "    kfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "\n",
    "    for fold, (train_ids, val_ids) in enumerate(kfold.split(train_dataset)):\n",
    "        print(f'Fold {fold+1}/{k_folds}')\n",
    "\n",
    "        # Create data loaders for this fold with custom collate function\n",
    "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "        val_subsampler = torch.utils.data.SubsetRandomSampler(val_ids)\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=1,\n",
    "            sampler=train_subsampler,\n",
    "            num_workers=8,\n",
    "            pin_memory=True,\n",
    "            collate_fn=custom_collate_fn\n",
    "        )\n",
    "\n",
    "        val_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=1,\n",
    "            sampler=val_subsampler,\n",
    "            num_workers=8,\n",
    "            pin_memory=True,\n",
    "            collate_fn=custom_collate_fn\n",
    "        )\n",
    "\n",
    "        # Initialize the model and move it to the correct device\n",
    "        model = MultiSeriesSpineModel(num_conditions=len(train_dataset.label_columns), num_classes=3)\n",
    "        model = model.to(device)\n",
    "\n",
    "        # Initialize optimizer and loss functions\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "        early_stopping = EarlyStopping(patience=3)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            total_train_loss = 0.0\n",
    "\n",
    "            # Training loop\n",
    "            for batch in tqdm(train_loader, desc=f\"Fold {fold+1} Epoch {epoch+1}/{num_epochs}\", unit=\"batch\"):\n",
    "                try:\n",
    "                    # Extract images, labels, and attention masks\n",
    "                    images = batch['images']\n",
    "                    labels = batch['labels'].to(device)  # Shape: [batch_size, num_conditions]\n",
    "                    attention_masks = batch['attention_masks']\n",
    "\n",
    "                    # Process each series\n",
    "                    sagittal_t1 = images['Sagittal T1']  # Shape: [batch_size, num_slices, H, W]\n",
    "                    sagittal_t2_stir = images['Sagittal T2_STIR']\n",
    "                    axial_t2 = images['Axial T2']\n",
    "\n",
    "                    # Resample slices\n",
    "                    sagittal_t1 = [resample_slices(img.squeeze(0), target_slices=10) for img in sagittal_t1]\n",
    "                    sagittal_t2_stir = [resample_slices(img.squeeze(0), target_slices=10) for img in sagittal_t2_stir]\n",
    "                    axial_t2 = [resample_slices(img.squeeze(0), target_slices=10) for img in axial_t2]\n",
    "\n",
    "                    # Check shapes before stacking\n",
    "                    for img in sagittal_t1:\n",
    "                        if img.shape != (10, 512, 512):\n",
    "                            print(f\"Invalid image shape in sagittal_t1: {img.shape}\")\n",
    "                            raise ValueError(\"Invalid image shape in sagittal_t1\")\n",
    "                    for img in sagittal_t2_stir:\n",
    "                        if img.shape != (10, 512, 512):\n",
    "                            print(f\"Invalid image shape in sagittal_t2_stir: {img.shape}\")\n",
    "                            raise ValueError(\"Invalid image shape in sagittal_t2_stir\")\n",
    "                    for img in axial_t2:\n",
    "                        if img.shape != (10, 512, 512):\n",
    "                            print(f\"Invalid image shape in axial_t2: {img.shape}\")\n",
    "                            raise ValueError(\"Invalid image shape in axial_t2\")\n",
    "\n",
    "                    # Stack slices into the channel dimension\n",
    "                    sagittal_t1 = torch.stack([img.reshape(-1, 512, 512) for img in sagittal_t1]).to(device)\n",
    "                    sagittal_t2_stir = torch.stack([img.reshape(-1, 512, 512) for img in sagittal_t2_stir]).to(device)\n",
    "                    axial_t2 = torch.stack([img.reshape(-1, 512, 512) for img in axial_t2]).to(device)\n",
    "\n",
    "                    # Move attention masks to the same device\n",
    "                    mask_t1 = attention_masks['Sagittal T1'].to(device)  # Shape: [batch_size, num_slices, H, W]\n",
    "                    mask_t2_stir = attention_masks['Sagittal T2_STIR'].to(device)\n",
    "                    mask_axial = attention_masks['Axial T2'].to(device)\n",
    "\n",
    "                    # Combine masks across slices (max over slices)\n",
    "                    gt_mask_t1 = torch.max(mask_t1, dim=1)[0].unsqueeze(1)  # Shape: [batch_size, 1, H, W]\n",
    "                    gt_mask_t2_stir = torch.max(mask_t2_stir, dim=1)[0].unsqueeze(1)\n",
    "                    gt_mask_axial = torch.max(mask_axial, dim=1)[0].unsqueeze(1)\n",
    "\n",
    "                    # Forward pass\n",
    "                    outputs, attention_maps = model(sagittal_t1, sagittal_t2_stir, axial_t2)\n",
    "\n",
    "                    # Compute the custom loss\n",
    "                    classification_loss = custom_loss(outputs, labels, any_severe_scalar=any_severe_scalar)\n",
    "\n",
    "                    # Compute attention loss\n",
    "                    attention_loss = 0.0\n",
    "                    attention_criterion = nn.MSELoss()\n",
    "                    for attention_map, gt_mask in zip(attention_maps, [gt_mask_t1, gt_mask_t2_stir, gt_mask_axial]):\n",
    "                        # Upsample the attention map to match the ground truth mask size\n",
    "                        attention_map_upsampled = F.interpolate(attention_map, size=gt_mask.shape[2:], mode='bilinear', align_corners=False)\n",
    "                        attention_loss += attention_criterion(attention_map_upsampled, gt_mask)\n",
    "\n",
    "                    # Total loss\n",
    "                    total_loss = classification_loss + lambda_attention * attention_loss\n",
    "\n",
    "                    # Zero gradients\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # Backpropagation and optimization\n",
    "                    total_loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    total_train_loss += total_loss.item()\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing batch during training: {e}\")\n",
    "                    continue  # Skip this batch\n",
    "\n",
    "            avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "            # Validation loop\n",
    "            model.eval()\n",
    "            total_val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for batch in val_loader:\n",
    "                    try:\n",
    "                        images = batch['images']\n",
    "                        labels = batch['labels'].to(device)\n",
    "                        attention_masks = batch['attention_masks']\n",
    "                \n",
    "                        # Preprocess images\n",
    "                        sagittal_t1 = [resample_slices(img.squeeze(0), target_slices=10) for img in images['Sagittal T1']]\n",
    "                        sagittal_t2_stir = [resample_slices(img.squeeze(0), target_slices=10) for img in images['Sagittal T2_STIR']]\n",
    "                        axial_t2 = [resample_slices(img.squeeze(0), target_slices=10) for img in images['Axial T2']]\n",
    "\n",
    "                        # Check shapes before stacking\n",
    "                        for img in sagittal_t1:\n",
    "                            if img.shape != (10, 512, 512):\n",
    "                                print(f\"Invalid image shape in sagittal_t1 (validation): {img.shape}\")\n",
    "                                raise ValueError(\"Invalid image shape in sagittal_t1 (validation)\")\n",
    "                        for img in sagittal_t2_stir:\n",
    "                            if img.shape != (10, 512, 512):\n",
    "                                print(f\"Invalid image shape in sagittal_t2_stir (validation): {img.shape}\")\n",
    "                                raise ValueError(\"Invalid image shape in sagittal_t2_stir (validation)\")\n",
    "                        for img in axial_t2:\n",
    "                            if img.shape != (10, 512, 512):\n",
    "                                print(f\"Invalid image shape in axial_t2 (validation): {img.shape}\")\n",
    "                                raise ValueError(\"Invalid image shape in axial_t2 (validation)\")\n",
    "\n",
    "                        sagittal_t1 = torch.stack([img.reshape(-1, 512, 512) for img in sagittal_t1]).to(device)\n",
    "                        sagittal_t2_stir = torch.stack([img.reshape(-1, 512, 512) for img in sagittal_t2_stir]).to(device)\n",
    "                        axial_t2 = torch.stack([img.reshape(-1, 512, 512) for img in axial_t2]).to(device)\n",
    "                \n",
    "                        mask_t1 = attention_masks['Sagittal T1'].to(device)  # Shape: [batch_size, num_slices, H, W]\n",
    "                        mask_t2_stir = attention_masks['Sagittal T2_STIR'].to(device)\n",
    "                        mask_axial = attention_masks['Axial T2'].to(device)\n",
    "                \n",
    "                        # Combine masks across slices (max over slices)\n",
    "                        gt_mask_t1 = torch.max(mask_t1, dim=1)[0].unsqueeze(1)  # Shape: [batch_size, 1, H, W]\n",
    "                        gt_mask_t2_stir = torch.max(mask_t2_stir, dim=1)[0].unsqueeze(1)\n",
    "                        gt_mask_axial = torch.max(mask_axial, dim=1)[0].unsqueeze(1)\n",
    "                \n",
    "                        outputs, attention_maps = model(sagittal_t1, sagittal_t2_stir, axial_t2)\n",
    "                \n",
    "                        classification_loss = custom_loss(outputs, labels, any_severe_scalar=any_severe_scalar)\n",
    "                \n",
    "                        attention_loss = 0.0\n",
    "                        attention_criterion = nn.MSELoss()\n",
    "                        for attention_map, gt_mask in zip(attention_maps, [gt_mask_t1, gt_mask_t2_stir, gt_mask_axial]):\n",
    "                            # Upsample the attention map to match the ground truth mask size\n",
    "                            attention_map_upsampled = F.interpolate(\n",
    "                                attention_map,\n",
    "                                size=gt_mask.shape[2:],  # This will be [H, W]\n",
    "                                mode='bilinear',\n",
    "                                align_corners=False\n",
    "                            )\n",
    "                            attention_loss += attention_criterion(attention_map_upsampled, gt_mask)\n",
    "                \n",
    "                        total_loss = classification_loss + lambda_attention * attention_loss\n",
    "                \n",
    "                        total_val_loss += total_loss.item()\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing batch during validation: {e}\")\n",
    "                        continue  # Skip this batch\n",
    "                \n",
    "                avg_val_loss = total_val_loss / len(val_loader) if len(val_loader) > 0 else float('inf')\n",
    "            \n",
    "                print(f\"Fold {fold+1} Epoch [{epoch+1}/{num_epochs}] Avg Train Loss: {avg_train_loss:.4f}, Avg Val Loss: {avg_val_loss:.4f}\")\n",
    "            \n",
    "                # Early stopping based on validation loss\n",
    "                early_stopping(avg_val_loss, model)\n",
    "                if early_stopping.early_stop:\n",
    "                    print(f\"Early stopping triggered for Fold {fold+1}!\")\n",
    "                    break\n",
    "\n",
    "\n",
    "        # Save the model for this fold\n",
    "        torch.save(model.state_dict(), f'model_fold_{fold+1}.pth')\n",
    "        print(f\"Completed Fold {fold+1}/{k_folds}. Model saved to model_fold_{fold+1}.pth.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b316883f-d2c8-418d-aac1-a32000f56967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Fold 1 Epoch 1/5:   7%|â–‹         | 117/1580 [02:52<07:53,  3.09batch/s]  "
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "\n",
    "n_folds = 5\n",
    "\n",
    "train_k_fold_with_custom_loss(train_dataset, k_folds=n_folds, num_epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94613ed0-a180-4284-8c66-c9a85780c610",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a602b6b-2f47-4664-88d3-c13c0ff1a80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class EnsembleModel(nn.Module):\n",
    "    def __init__(self, model_class, model_paths, device):\n",
    "        super(EnsembleModel, self).__init__()\n",
    "        self.models = nn.ModuleList()\n",
    "        for path in model_paths:\n",
    "            model = model_class()\n",
    "            model.load_state_dict(torch.load(path, map_location=device))\n",
    "            model.to(device)\n",
    "            model.eval()  # Set model to evaluation mode\n",
    "            self.models.append(model)\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, sagittal_t1, sagittal_t2_stir, axial_t2):\n",
    "        outputs_list = []\n",
    "        for model in self.models:\n",
    "            outputs, _ = model(sagittal_t1, sagittal_t2_stir, axial_t2)\n",
    "            outputs_list.append(outputs)\n",
    "        # Stack outputs and take mean over the ensemble dimension\n",
    "        outputs = torch.stack(outputs_list, dim=0)\n",
    "        avg_outputs = torch.mean(outputs, dim=0)\n",
    "        return avg_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a8a504f-4591-449e-8906-5dda904b962e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EnsembleModel(\n",
       "  (models): ModuleList(\n",
       "    (0-4): 5 x MultiSeriesSpineModel(\n",
       "      (cnn_sagittal_t1): ResNetFeatureExtractor(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(10, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "          (4): Sequential(\n",
       "            (0): BasicBlock(\n",
       "              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (1): BasicBlock(\n",
       "              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (5): Sequential(\n",
       "            (0): BasicBlock(\n",
       "              (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (downsample): Sequential(\n",
       "                (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (1): BasicBlock(\n",
       "              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (6): Sequential(\n",
       "            (0): BasicBlock(\n",
       "              (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (downsample): Sequential(\n",
       "                (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (1): BasicBlock(\n",
       "              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (7): Sequential(\n",
       "            (0): BasicBlock(\n",
       "              (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (downsample): Sequential(\n",
       "                (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (1): BasicBlock(\n",
       "              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (cnn_sagittal_t2_stir): ResNetFeatureExtractor(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(10, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "          (4): Sequential(\n",
       "            (0): BasicBlock(\n",
       "              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (1): BasicBlock(\n",
       "              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (5): Sequential(\n",
       "            (0): BasicBlock(\n",
       "              (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (downsample): Sequential(\n",
       "                (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (1): BasicBlock(\n",
       "              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (6): Sequential(\n",
       "            (0): BasicBlock(\n",
       "              (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (downsample): Sequential(\n",
       "                (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (1): BasicBlock(\n",
       "              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (7): Sequential(\n",
       "            (0): BasicBlock(\n",
       "              (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (downsample): Sequential(\n",
       "                (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (1): BasicBlock(\n",
       "              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (cnn_axial_t2): ResNetFeatureExtractor(\n",
       "        (features): Sequential(\n",
       "          (0): Conv2d(10, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "          (4): Sequential(\n",
       "            (0): BasicBlock(\n",
       "              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (1): BasicBlock(\n",
       "              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (5): Sequential(\n",
       "            (0): BasicBlock(\n",
       "              (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (downsample): Sequential(\n",
       "                (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (1): BasicBlock(\n",
       "              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (6): Sequential(\n",
       "            (0): BasicBlock(\n",
       "              (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (downsample): Sequential(\n",
       "                (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (1): BasicBlock(\n",
       "              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (7): Sequential(\n",
       "            (0): BasicBlock(\n",
       "              (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (downsample): Sequential(\n",
       "                (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (1): BasicBlock(\n",
       "              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (relu): ReLU(inplace=True)\n",
       "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (attention_sagittal_t1): Sequential(\n",
       "        (0): Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): Sigmoid()\n",
       "      )\n",
       "      (attention_sagittal_t2_stir): Sequential(\n",
       "        (0): Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): Sigmoid()\n",
       "      )\n",
       "      (attention_axial_t2): Sequential(\n",
       "        (0): Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): Sigmoid()\n",
       "      )\n",
       "      (fc1): Linear(in_features=1536, out_features=512, bias=True)\n",
       "      (fc2): Linear(in_features=512, out_features=75, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "k_folds = n_folds\n",
    "model_paths = [f'model_fold_{i+1}.pth' for i in range(k_folds)]\n",
    "\n",
    "ensemble_model = EnsembleModel(\n",
    "    model_class=lambda: MultiSeriesSpineModel(num_conditions=len(LABELS), num_classes=3),\n",
    "    model_paths=model_paths,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "ensemble_model.to(device)\n",
    "ensemble_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9714202-3dcf-4676-b2bd-e937e22e2ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(ensemble_model.state_dict(), 'ensemble_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89be133-6772-48b9-897f-67dabbd47e4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
