{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f5bce19-3eaa-4221-a7d9-9aa696f67de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-30 18:42:42.870386: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-09-30 18:42:42.884103: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-30 18:42:42.901847: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-30 18:42:42.907330: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-30 18:42:42.920099: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from torchvision import transforms, models\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import models\n",
    "from sklearn.model_selection import KFold\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import traceback\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba5a74c8-50ad-4a7a-abff-1d44e32fd706",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1249/1497933176.py:29: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df.replace(label2id, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define constants\n",
    "SERIES_DESCRIPTIONS = ['Sagittal T1', 'Sagittal T2_STIR', 'Axial T2']\n",
    "CONDITIONS = [\n",
    "    'spinal_canal_stenosis', \n",
    "    'left_neural_foraminal_narrowing', \n",
    "    'right_neural_foraminal_narrowing',\n",
    "    'left_subarticular_stenosis',\n",
    "    'right_subarticular_stenosis'\n",
    "]\n",
    "LEVELS = [\n",
    "    'l1_l2',\n",
    "    'l2_l3',\n",
    "    'l3_l4',\n",
    "    'l4_l5',\n",
    "    'l5_s1',\n",
    "]\n",
    "LABELS = [f'{condition}_{level}' for condition in CONDITIONS for level in LEVELS]\n",
    "\n",
    "# Set the root directory for your Kaggle files\n",
    "rd = './kaggle-files'\n",
    "\n",
    "# Load the main CSV file\n",
    "df = pd.read_csv(f'{rd}/train.csv')\n",
    "\n",
    "df = df.fillna(-100)  # Use -100 to indicate missing labels\n",
    "\n",
    "# Map the labels to integers for multi-class classification\n",
    "label2id = {'Normal/Mild': 0, 'Moderate': 1, 'Severe': 2}\n",
    "df.replace(label2id, inplace=True)\n",
    "\n",
    "# Load the coordinates data\n",
    "coordinates_df = pd.read_csv(f'{rd}/dfc_updated.csv')\n",
    "# Keep only rows where 'slice_number' is not NaN\n",
    "coordinates_df = coordinates_df.dropna(subset=['slice_number'])\n",
    "coordinates_df['slice_number'] = coordinates_df['slice_number'].astype(int)\n",
    "\n",
    "# Load the series descriptions\n",
    "series_description_df = pd.read_csv(f'{rd}/train_series_descriptions.csv')\n",
    "series_description_df['series_description'] = series_description_df['series_description'].str.replace('T2/STIR', 'T2_STIR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b50b818-381c-4d63-9e96-5f8cfcd02c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LumbarSpineDataset(Dataset):\n",
    "    def __init__(self, df, coordinates_df, series_description_df, root_dir, transform=None):\n",
    "        self.df = df\n",
    "        self.coordinates_df = coordinates_df\n",
    "        self.series_description_df = series_description_df\n",
    "        self.root_dir = root_dir  # The root directory where images are stored\n",
    "        self.transform = transform\n",
    "\n",
    "        # Get the list of study_ids\n",
    "        self.study_ids = self.df['study_id'].unique()\n",
    "\n",
    "        # List of label columns\n",
    "        self.label_columns = [col for col in df.columns if col != 'study_id']\n",
    "\n",
    "        # Prepare a mapping for images and annotations\n",
    "        self.study_image_paths = self._prepare_image_paths()\n",
    "\n",
    "        # Create a mapping from study_id to labels\n",
    "        self.labels_dict = self._prepare_labels()\n",
    "\n",
    "    def _prepare_image_paths(self):\n",
    "        study_image_paths = {}\n",
    "        for study_id in self.study_ids:\n",
    "            study_image_paths[study_id] = {}\n",
    "            for series_description in SERIES_DESCRIPTIONS:\n",
    "                series_description_clean = series_description.replace('/', '_')\n",
    "                image_dir = os.path.join(self.root_dir, 'cvt_png', str(study_id), series_description_clean)\n",
    "                if os.path.exists(image_dir):\n",
    "                    # Get all images in the directory\n",
    "                    image_paths = sorted(glob.glob(os.path.join(image_dir, '*.png')))\n",
    "                    study_image_paths[study_id][series_description] = image_paths\n",
    "                else:\n",
    "                    # Handle missing series\n",
    "                    study_image_paths[study_id][series_description] = []\n",
    "        return study_image_paths\n",
    "\n",
    "    def _prepare_labels(self):\n",
    "        labels_dict = {}\n",
    "        for idx, row in self.df.iterrows():\n",
    "            study_id = row['study_id']\n",
    "            labels = []\n",
    "            for col in self.label_columns:\n",
    "                label = row[col]\n",
    "                if pd.isnull(label) or label == -100:\n",
    "                    label = -100  # Use -100 for missing labels (ignore_index)\n",
    "                else:\n",
    "                    label = int(label)\n",
    "                labels.append(label)\n",
    "            labels_dict[study_id] = labels\n",
    "        return labels_dict\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.study_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        study_id = self.study_ids[idx]\n",
    "        images = {}\n",
    "        annotations = {}\n",
    "\n",
    "        # Load images for each series description\n",
    "        for series_description in SERIES_DESCRIPTIONS:\n",
    "            image_paths = self.study_image_paths[study_id][series_description]\n",
    "            series_images = []\n",
    "            for img_path in image_paths:\n",
    "                img = Image.open(img_path).convert('L')  # Convert to grayscale\n",
    "                if self.transform:\n",
    "                    img = self.transform(img)  # Shape: [1, H, W]\n",
    "                    img = img.squeeze(0)  # Remove the channel dimension, resulting in [H, W]\n",
    "                series_images.append(img)\n",
    "            if series_images:\n",
    "                series_tensor = torch.stack(series_images, dim=0)  # Shape: [num_slices, H, W]\n",
    "            else:\n",
    "                series_tensor = torch.zeros((1, 512, 512))  # Placeholder tensor\n",
    "            images[series_description] = series_tensor  # Shape: [num_slices, H, W]\n",
    "\n",
    "        # Get labels for the study_id\n",
    "        labels = self.labels_dict[study_id]\n",
    "        labels_tensor = torch.tensor(labels, dtype=torch.long)  # Use long dtype for CrossEntropyLoss\n",
    "\n",
    "        # Generate attention masks, default to zeros if no annotations\n",
    "        attention_masks = {}\n",
    "        for series_description in SERIES_DESCRIPTIONS:\n",
    "            series_tensor = images[series_description]\n",
    "            num_slices = series_tensor.shape[0]\n",
    "            masks = []\n",
    "            for slice_idx in range(num_slices):\n",
    "                image_shape = series_tensor[slice_idx].shape  # Get (H, W)\n",
    "                mask = torch.zeros(image_shape, dtype=torch.float32)  # Default to zero mask\n",
    "                # If annotations exist, generate the attention mask\n",
    "                study_annotations = self.coordinates_df[self.coordinates_df['study_id'] == study_id]\n",
    "                for _, row in study_annotations.iterrows():\n",
    "                    if row['series_description'] == series_description:\n",
    "                        x_pixel = int(row['x_scaled'] * image_shape[1])\n",
    "                        y_pixel = int(row['y_scaled'] * image_shape[0])\n",
    "                        sigma = 5  # Adjust sigma\n",
    "                        y_grid, x_grid = torch.meshgrid(\n",
    "                            torch.arange(image_shape[0], dtype=torch.float32),\n",
    "                            torch.arange(image_shape[1], dtype=torch.float32),\n",
    "                            indexing='ij'\n",
    "                        )\n",
    "                        gauss = torch.exp(-((x_grid - x_pixel) ** 2 + (y_grid - y_pixel) ** 2) / (2 * sigma ** 2))\n",
    "                        mask = torch.maximum(mask, gauss)\n",
    "                masks.append(mask)\n",
    "            attention_masks[series_description] = torch.stack(masks, dim=0)  # Shape: [num_slices, H, W]\n",
    "\n",
    "        sample = {\n",
    "            'study_id': study_id,\n",
    "            'images': images,\n",
    "            'labels': labels_tensor,\n",
    "            'attention_masks': attention_masks\n",
    "        }\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4855d643-ca6a-47d3-bfc1-449459363c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define any transformations if needed\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # Adjust mean and std if necessary\n",
    "])\n",
    "\n",
    "# Instantiate the dataset\n",
    "train_dataset = LumbarSpineDataset(\n",
    "    df=df,\n",
    "    coordinates_df=coordinates_df,\n",
    "    series_description_df=series_description_df,\n",
    "    root_dir='./rsna_output',  # Adjust the path as needed\n",
    "    transform=transform\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae09e7b2-843d-44fd-8689-6539fcb967d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_slices(image_tensor, target_slices=10):\n",
    "    # Ensure the image tensor has at least 3 dimensions\n",
    "    if image_tensor.dim() == 2:\n",
    "        image_tensor = image_tensor.unsqueeze(0)  # Add slice dimension\n",
    "    current_slices = image_tensor.shape[0]\n",
    "    if current_slices == target_slices:\n",
    "        return image_tensor  # No need to resample\n",
    "    if current_slices > target_slices:\n",
    "        indices = torch.linspace(0, current_slices - 1, target_slices).long()\n",
    "        return image_tensor[indices]\n",
    "    # If fewer slices, upsample\n",
    "    image_tensor = image_tensor.unsqueeze(0).unsqueeze(0)  # Shape: [1, 1, num_slices, H, W]\n",
    "    image_tensor_resized = F.interpolate(\n",
    "        image_tensor,\n",
    "        size=(target_slices, image_tensor.shape[3], image_tensor.shape[4]),\n",
    "        mode='trilinear',\n",
    "        align_corners=False\n",
    "    )\n",
    "    image_tensor_resized = image_tensor_resized.squeeze(0).squeeze(0)  # Shape: [target_slices, H, W]\n",
    "    return image_tensor_resized\n",
    "\n",
    "# Early Stopping class\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, delta=0, path='checkpoint.pth'):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.best_score = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "        self.best_loss = float('inf')\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Save model when validation loss decreases.'''\n",
    "        if val_loss < self.best_loss:\n",
    "            self.best_loss = val_loss\n",
    "            torch.save(model.state_dict(), self.path)\n",
    "            print(f\"Validation loss decreased ({self.best_loss:.4f}). Saving model to {self.path}\")\n",
    "\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Save model when validation loss decreases.'''\n",
    "        self.best_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'checkpoint.pth')\n",
    "\n",
    "# Define the ResNet feature extractor\n",
    "class ResNetFeatureExtractor(nn.Module):\n",
    "    def __init__(self, in_channels=10):\n",
    "        super(ResNetFeatureExtractor, self).__init__()\n",
    "        resnet = models.resnet18(pretrained=True)\n",
    "\n",
    "        # Modify the first convolutional layer to accept in_channels\n",
    "        resnet.conv1 = nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "        # Extract layers up to layer4 (exclude avgpool and fc layers)\n",
    "        self.features = nn.Sequential(\n",
    "            resnet.conv1,\n",
    "            resnet.bn1,\n",
    "            resnet.relu,\n",
    "            resnet.maxpool,\n",
    "            resnet.layer1,\n",
    "            resnet.layer2,\n",
    "            resnet.layer3,\n",
    "            resnet.layer4\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return x\n",
    "\n",
    "# Define the main model\n",
    "class MultiSeriesSpineModel(nn.Module):\n",
    "    def __init__(self, num_conditions=25, num_classes=3):\n",
    "        super(MultiSeriesSpineModel, self).__init__()\n",
    "        self.num_conditions = num_conditions\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Feature extractors for each MRI series\n",
    "        self.cnn_sagittal_t1 = ResNetFeatureExtractor(in_channels=10)\n",
    "        self.cnn_sagittal_t2_stir = ResNetFeatureExtractor(in_channels=10)\n",
    "        self.cnn_axial_t2 = ResNetFeatureExtractor(in_channels=10)\n",
    "\n",
    "        # Define attention layers for each series\n",
    "        self.attention_sagittal_t1 = nn.Sequential(\n",
    "            nn.Conv2d(512, 1, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.attention_sagittal_t2_stir = nn.Sequential(\n",
    "            nn.Conv2d(512, 1, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.attention_axial_t2 = nn.Sequential(\n",
    "            nn.Conv2d(512, 1, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # Define the final classification layers\n",
    "        combined_feature_size = 512 * 3  # Since we're concatenating features from three models\n",
    "\n",
    "        self.fc1 = nn.Linear(combined_feature_size, 512)\n",
    "        self.fc2 = nn.Linear(512, num_conditions * num_classes)  # Output layer\n",
    "\n",
    "    def forward(self, sagittal_t1, sagittal_t2_stir, axial_t2):\n",
    "        # The tensors are of shape [batch_size, in_channels, H, W]\n",
    "        features_sagittal_t1 = self.cnn_sagittal_t1(sagittal_t1)  # Shape: [batch_size, 512, H, W]\n",
    "        features_sagittal_t2_stir = self.cnn_sagittal_t2_stir(sagittal_t2_stir)\n",
    "        features_axial_t2 = self.cnn_axial_t2(axial_t2)\n",
    "\n",
    "        # Generate attention maps (learned by the model)\n",
    "        attention_map_t1 = self.attention_sagittal_t1(features_sagittal_t1)  # Shape: [batch_size, 1, H, W]\n",
    "        attention_map_t2_stir = self.attention_sagittal_t2_stir(features_sagittal_t2_stir)\n",
    "        attention_map_axial = self.attention_axial_t2(features_axial_t2)\n",
    "\n",
    "        # Apply attention\n",
    "        attended_features_t1 = features_sagittal_t1 * attention_map_t1  # Element-wise multiplication\n",
    "        attended_features_t2_stir = features_sagittal_t2_stir * attention_map_t2_stir\n",
    "        attended_features_axial = features_axial_t2 * attention_map_axial\n",
    "\n",
    "        # Global average pooling\n",
    "        features_sagittal_t1 = F.adaptive_avg_pool2d(attended_features_t1, (1, 1)).view(attended_features_t1.size(0), -1)\n",
    "        features_sagittal_t2_stir = F.adaptive_avg_pool2d(attended_features_t2_stir, (1, 1)).view(attended_features_t2_stir.size(0), -1)\n",
    "        features_axial_t2 = F.adaptive_avg_pool2d(attended_features_axial, (1, 1)).view(attended_features_axial.size(0), -1)\n",
    "\n",
    "        # Concatenate features\n",
    "        combined_features = torch.cat([features_sagittal_t1, features_sagittal_t2_stir, features_axial_t2], dim=1)\n",
    "\n",
    "        # Pass through final classification layers\n",
    "        x = F.relu(self.fc1(combined_features))\n",
    "        x = self.fc2(x)  # Shape: [batch_size, num_conditions * num_classes]\n",
    "        x = x.view(-1, self.num_conditions, self.num_classes)  # Reshape to [batch_size, num_conditions, num_classes]\n",
    "\n",
    "        return x, [attention_map_t1, attention_map_t2_stir, attention_map_axial]\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e121f58-79ad-465a-bfd9-4f6aac25abf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def custom_loss(outputs, labels, any_severe_scalar=0.5):\n",
    "    \"\"\"\n",
    "    Custom loss function that mirrors the Kaggle evaluation metric.\n",
    "\n",
    "    Args:\n",
    "        outputs (torch.Tensor): Model predictions of shape [batch_size, num_conditions * num_levels, num_classes=3].\n",
    "        labels (torch.Tensor): Ground truth labels of shape [batch_size, num_conditions * num_levels] with values {0,1,2} or -100 for missing.\n",
    "        any_severe_scalar (float): Scalar to weight the 'any_severe_spinal' loss component.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (total_loss, per_condition_loss_dict)\n",
    "    \"\"\"\n",
    "    # Define sample weights: 1 for normal/mild (0), 2 for moderate (1), 4 for severe (2)\n",
    "    sample_weights = torch.ones_like(labels, dtype=torch.float, device=labels.device)\n",
    "    sample_weights[labels == 0] = 1.0\n",
    "    sample_weights[labels == 1] = 2.0\n",
    "    sample_weights[labels == 2] = 4.0\n",
    "    # Assign zero weight to ignored labels\n",
    "    sample_weights[labels == -100] = 0.0\n",
    "\n",
    "    # Flatten outputs and labels for easier processing\n",
    "    batch_size, num_labels, num_classes = outputs.shape\n",
    "    outputs_flat = outputs.view(-1, num_classes)  # [batch_size * num_conditions * num_levels, 3]\n",
    "    labels_flat = labels.view(-1)                # [batch_size * num_conditions * num_levels]\n",
    "\n",
    "    # Compute log probabilities\n",
    "    log_probs = F.log_softmax(outputs_flat, dim=1)  # [N, 3]\n",
    "\n",
    "    # Mask to filter out ignored labels\n",
    "    valid_mask = labels_flat != -100\n",
    "    labels_valid = labels_flat[valid_mask]          # [num_valid]\n",
    "    log_probs_valid = log_probs[valid_mask, labels_valid.long()]  # [num_valid]\n",
    "\n",
    "    # Get sample weights for valid labels\n",
    "    weights_valid = sample_weights.view(-1)[valid_mask]  # [num_valid]\n",
    "\n",
    "    # Compute weighted negative log likelihood for main labels\n",
    "    loss_main = -(log_probs_valid * weights_valid).mean()\n",
    "\n",
    "    # ----- Any Severe Spinal Prediction -----\n",
    "    # Identify indices corresponding to 'spinal_canal_stenosis'\n",
    "    spinal_condition = 'spinal_canal_stenosis'\n",
    "    spinal_indices = [i for i, label in enumerate(LABELS) if label.startswith(spinal_condition)]\n",
    "\n",
    "    if not spinal_indices:\n",
    "        raise ValueError(\"No labels found for 'spinal_canal_stenosis' in LABELS.\")\n",
    "\n",
    "    # Extract outputs and labels for spinal condition\n",
    "    outputs_spinal = outputs[:, spinal_indices, :]  # [batch_size, num_spinal, 3]\n",
    "    labels_spinal = labels[:, spinal_indices]       # [batch_size, num_spinal]\n",
    "\n",
    "    # Compute ground truth for 'any_severe_spinal': 1 if any spinal label is severe, else 0\n",
    "    any_severe_label = (labels_spinal == 2).float().max(dim=1)[0]  # [batch_size]\n",
    "\n",
    "    # Compute predicted probability for 'severe' class\n",
    "    prob_severe = F.softmax(outputs_spinal, dim=2)[:, :, 2]  # [batch_size, num_spinal]\n",
    "\n",
    "    # 'any_severe_pred' is the maximum probability of severe across spinal labels\n",
    "    any_severe_pred = prob_severe.max(dim=1)[0]  # [batch_size]\n",
    "\n",
    "    # Compute binary cross-entropy loss for 'any_severe_spinal'\n",
    "    loss_any_severe = F.binary_cross_entropy(any_severe_pred, any_severe_label, reduction='mean')\n",
    "\n",
    "    # ----- Per-Condition Losses -----\n",
    "    # Initialize a dictionary to store per-condition losses\n",
    "    per_condition_loss = {}\n",
    "\n",
    "    for condition in ['spinal', 'foraminal', 'subarticular']:\n",
    "        condition_indices = [i for i, label in enumerate(LABELS) if label.startswith(condition)]\n",
    "        if not condition_indices:\n",
    "            per_condition_loss[condition] = 0.0\n",
    "            continue  # Skip if no labels for this condition\n",
    "\n",
    "        outputs_condition = outputs[:, condition_indices, :]  # [batch_size, num_condition_labels, 3]\n",
    "        labels_condition = labels[:, condition_indices]       # [batch_size, num_condition_labels]\n",
    "\n",
    "        # Flatten for loss computation\n",
    "        outputs_condition_flat = outputs_condition.view(-1, num_classes)\n",
    "        labels_condition_flat = labels_condition.view(-1)     # [batch_size * num_condition_labels]\n",
    "\n",
    "        # Compute log probabilities\n",
    "        log_probs_condition = F.log_softmax(outputs_condition_flat, dim=1)  # [N, 3]\n",
    "\n",
    "        # Mask to filter out ignored labels\n",
    "        valid_mask_condition = labels_condition_flat != -100\n",
    "        labels_valid_condition = labels_condition_flat[valid_mask_condition]          # [num_valid_condition]\n",
    "        log_probs_valid_condition = log_probs_condition[valid_mask_condition, labels_valid_condition.long()]  # [num_valid_condition]\n",
    "\n",
    "        # Get sample weights for valid labels\n",
    "        condition_sample_weights = sample_weights.view(-1)[condition_indices]          # [num_condition_labels]\n",
    "        weights_valid_condition = condition_sample_weights.view(-1)[valid_mask_condition]  # [num_valid_condition]\n",
    "\n",
    "        # Compute weighted negative log likelihood\n",
    "        if weights_valid_condition.numel() > 0:\n",
    "            loss_condition = -(log_probs_valid_condition * weights_valid_condition).mean()\n",
    "            per_condition_loss[condition] = loss_condition.item()\n",
    "        else:\n",
    "            per_condition_loss[condition] = 0.0\n",
    "\n",
    "    # ----- Combine Losses -----\n",
    "    total_loss = loss_main + any_severe_scalar * loss_any_severe\n",
    "    per_condition_loss['total_loss'] = loss_main.item()\n",
    "    per_condition_loss['any_severe_loss'] = loss_any_severe.item()\n",
    "\n",
    "    return total_loss, per_condition_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42b30baa-9e89-4605-95b4-e48a117d9993",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    collated_batch = {}\n",
    "    # Handle 'study_id' separately\n",
    "    collated_batch['study_id'] = [item['study_id'] for item in batch]\n",
    "    # Handle 'labels'\n",
    "    labels_list = []\n",
    "    for item in batch:\n",
    "        labels = item['labels']\n",
    "        if not isinstance(labels, torch.Tensor):\n",
    "            labels = torch.tensor(labels, dtype=torch.long)\n",
    "        if labels.dim() == 0:\n",
    "            labels = labels.unsqueeze(0)\n",
    "        labels_list.append(labels)\n",
    "    collated_batch['labels'] = torch.stack(labels_list)\n",
    "    # Handle 'images' and 'attention_masks'\n",
    "    for key in ['images', 'attention_masks']:\n",
    "        collated_batch[key] = {}\n",
    "        sub_keys = batch[0][key].keys()\n",
    "        for sub_key in sub_keys:\n",
    "            items_list = []\n",
    "            for item in batch:\n",
    "                data = item[key][sub_key]\n",
    "                if not isinstance(data, torch.Tensor):\n",
    "                    data = torch.tensor(data)\n",
    "                items_list.append(data)\n",
    "            collated_batch[key][sub_key] = torch.stack(items_list)\n",
    "    return collated_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31c948de-693d-4d5b-9be8-177639c6c1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# [Include your existing constants, Dataset class, model definitions, custom_loss, resample_slices, and EarlyStopping class here]\n",
    "\n",
    "def train_one_epoch(model, device, train_loader, optimizer, any_severe_scalar, lambda_attention, writer, fold, epoch, scheduler=None):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    train_losses = []\n",
    "    per_condition_losses = {'spinal': 0.0, 'foraminal': 0.0, 'subarticular': 0.0}\n",
    "    \n",
    "    for batch_idx, batch in enumerate(tqdm(train_loader, desc=f\"Fold {fold} Epoch {epoch}/Training\", unit=\"batch\")):\n",
    "        try:\n",
    "            # Extract images, labels, and attention masks\n",
    "            images = batch['images']\n",
    "            labels = batch['labels'].to(device)  # Shape: [batch_size, num_conditions]\n",
    "            attention_masks = batch['attention_masks']\n",
    "\n",
    "            # Process each series\n",
    "            sagittal_t1 = images['Sagittal T1']  # Shape: [batch_size, num_slices, H, W]\n",
    "            sagittal_t2_stir = images['Sagittal T2_STIR']\n",
    "            axial_t2 = images['Axial T2']\n",
    "\n",
    "            # Resample slices\n",
    "            sagittal_t1 = [resample_slices(img.squeeze(0), target_slices=10) for img in sagittal_t1]\n",
    "            sagittal_t2_stir = [resample_slices(img.squeeze(0), target_slices=10) for img in sagittal_t2_stir]\n",
    "            axial_t2 = [resample_slices(img.squeeze(0), target_slices=10) for img in axial_t2]\n",
    "\n",
    "            # Check shapes before stacking\n",
    "            for img in sagittal_t1:\n",
    "                if img.shape != (10, 512, 512):\n",
    "                    print(f\"Invalid image shape in sagittal_t1: {img.shape}\")\n",
    "                    raise ValueError(\"Invalid image shape in sagittal_t1\")\n",
    "            for img in sagittal_t2_stir:\n",
    "                if img.shape != (10, 512, 512):\n",
    "                    print(f\"Invalid image shape in sagittal_t2_stir: {img.shape}\")\n",
    "                    raise ValueError(\"Invalid image shape in sagittal_t2_stir\")\n",
    "            for img in axial_t2:\n",
    "                if img.shape != (10, 512, 512):\n",
    "                    print(f\"Invalid image shape in axial_t2: {img.shape}\")\n",
    "                    raise ValueError(\"Invalid image shape in axial_t2\")\n",
    "\n",
    "            # Stack slices into the channel dimension\n",
    "            sagittal_t1 = torch.stack([img.reshape(-1, 512, 512) for img in sagittal_t1]).to(device)\n",
    "            sagittal_t2_stir = torch.stack([img.reshape(-1, 512, 512) for img in sagittal_t2_stir]).to(device)\n",
    "            axial_t2 = torch.stack([img.reshape(-1, 512, 512) for img in axial_t2]).to(device)\n",
    "\n",
    "            # Move attention masks to the same device\n",
    "            mask_t1 = attention_masks['Sagittal T1'].to(device)  # Shape: [batch_size, num_slices, H, W]\n",
    "            mask_t2_stir = attention_masks['Sagittal T2_STIR'].to(device)\n",
    "            mask_axial = attention_masks['Axial T2'].to(device)\n",
    "\n",
    "            # Combine masks across slices (max over slices)\n",
    "            gt_mask_t1 = torch.max(mask_t1, dim=1)[0].unsqueeze(1)  # Shape: [batch_size, 1, H, W]\n",
    "            gt_mask_t2_stir = torch.max(mask_t2_stir, dim=1)[0].unsqueeze(1)\n",
    "            gt_mask_axial = torch.max(mask_axial, dim=1)[0].unsqueeze(1)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs, attention_maps = model(sagittal_t1, sagittal_t2_stir, axial_t2)\n",
    "\n",
    "            # Compute the custom loss\n",
    "            total_batch_loss, per_condition_loss = custom_loss(outputs, labels, any_severe_scalar=any_severe_scalar)\n",
    "\n",
    "            # Compute attention loss\n",
    "            attention_loss = 0.0\n",
    "            attention_criterion = nn.MSELoss()\n",
    "            for attention_map, gt_mask in zip(attention_maps, [gt_mask_t1, gt_mask_t2_stir, gt_mask_axial]):\n",
    "                # Upsample the attention map to match the ground truth mask size\n",
    "                attention_map_upsampled = F.interpolate(attention_map, size=gt_mask.shape[2:], mode='bilinear', align_corners=False)\n",
    "                attention_loss += attention_criterion(attention_map_upsampled, gt_mask)\n",
    "\n",
    "            # Total loss\n",
    "            total_batch_loss = total_batch_loss + lambda_attention * attention_loss\n",
    "\n",
    "            # Backpropagation and optimization\n",
    "            optimizer.zero_grad()\n",
    "            total_batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += total_batch_loss.item()\n",
    "            train_losses.append(total_batch_loss.item())\n",
    "            \n",
    "            # Accumulate per-condition losses\n",
    "            for condition in per_condition_loss:\n",
    "                if condition in per_condition_losses:\n",
    "                    per_condition_losses[condition] += per_condition_loss[condition]\n",
    "\n",
    "            # Log batch loss to TensorBoard every 100 batches\n",
    "            if (batch_idx + 1) % 100 == 0:\n",
    "                writer.add_scalar(f'Fold{fold}/Train_Batch_Loss', total_batch_loss.item(), epoch * len(train_loader) + batch_idx)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch during training: {e}\")\n",
    "            traceback.print_exc()\n",
    "            continue  # Skip this batch\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    writer.add_scalar(f'Fold{fold}/Train_Avg_Loss', avg_train_loss, epoch)\n",
    "    \n",
    "    # Log per-condition losses\n",
    "    for condition, loss in per_condition_losses.items():\n",
    "        avg_condition_loss = loss / len(train_loader)\n",
    "        writer.add_scalar(f'Fold{fold}/Train_{condition}_Loss', avg_condition_loss, epoch)\n",
    "\n",
    "    return avg_train_loss\n",
    "\n",
    "def validate_one_epoch(model, device, val_loader, any_severe_scalar, lambda_attention, writer, fold, epoch):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    val_losses = []\n",
    "    per_condition_losses = {'spinal': 0.0, 'foraminal': 0.0, 'subarticular': 0.0}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(tqdm(val_loader, desc=f\"Fold {fold} Epoch {epoch}/Validation\", unit=\"batch\")):\n",
    "            try:\n",
    "                images = batch['images']\n",
    "                labels = batch['labels'].to(device)\n",
    "                attention_masks = batch['attention_masks']\n",
    "        \n",
    "                # Preprocess images\n",
    "                sagittal_t1 = [resample_slices(img.squeeze(0), target_slices=10) for img in images['Sagittal T1']]\n",
    "                sagittal_t2_stir = [resample_slices(img.squeeze(0), target_slices=10) for img in images['Sagittal T2_STIR']]\n",
    "                axial_t2 = [resample_slices(img.squeeze(0), target_slices=10) for img in images['Axial T2']]\n",
    "\n",
    "                # Check shapes before stacking\n",
    "                for img in sagittal_t1:\n",
    "                    if img.shape != (10, 512, 512):\n",
    "                        print(f\"Invalid image shape in sagittal_t1 (validation): {img.shape}\")\n",
    "                        raise ValueError(\"Invalid image shape in sagittal_t1 (validation)\")\n",
    "                for img in sagittal_t2_stir:\n",
    "                    if img.shape != (10, 512, 512):\n",
    "                        print(f\"Invalid image shape in sagittal_t2_stir (validation): {img.shape}\")\n",
    "                        raise ValueError(\"Invalid image shape in sagittal_t2_stir (validation)\")\n",
    "                for img in axial_t2:\n",
    "                    if img.shape != (10, 512, 512):\n",
    "                        print(f\"Invalid image shape in axial_t2 (validation): {img.shape}\")\n",
    "                        raise ValueError(\"Invalid image shape in axial_t2 (validation)\")\n",
    "\n",
    "                sagittal_t1 = torch.stack([img.reshape(-1, 512, 512) for img in sagittal_t1]).to(device)\n",
    "                sagittal_t2_stir = torch.stack([img.reshape(-1, 512, 512) for img in sagittal_t2_stir]).to(device)\n",
    "                axial_t2 = torch.stack([img.reshape(-1, 512, 512) for img in axial_t2]).to(device)\n",
    "        \n",
    "                mask_t1 = attention_masks['Sagittal T1'].to(device)  # Shape: [batch_size, num_slices, H, W]\n",
    "                mask_t2_stir = attention_masks['Sagittal T2_STIR'].to(device)\n",
    "                mask_axial = attention_masks['Axial T2'].to(device)\n",
    "        \n",
    "                # Combine masks across slices (max over slices)\n",
    "                gt_mask_t1 = torch.max(mask_t1, dim=1)[0].unsqueeze(1)  # Shape: [batch_size, 1, H, W]\n",
    "                gt_mask_t2_stir = torch.max(mask_t2_stir, dim=1)[0].unsqueeze(1)\n",
    "                gt_mask_axial = torch.max(mask_axial, dim=1)[0].unsqueeze(1)\n",
    "        \n",
    "                # Forward pass\n",
    "                outputs, attention_maps = model(sagittal_t1, sagittal_t2_stir, axial_t2)\n",
    "\n",
    "                # Compute the custom loss\n",
    "                total_batch_loss, per_condition_loss = custom_loss(outputs, labels, any_severe_scalar=any_severe_scalar)\n",
    "\n",
    "                # Compute attention loss\n",
    "                attention_loss = 0.0\n",
    "                attention_criterion = nn.MSELoss()\n",
    "                for attention_map, gt_mask in zip(attention_maps, [gt_mask_t1, gt_mask_t2_stir, gt_mask_axial]):\n",
    "                    # Upsample the attention map to match the ground truth mask size\n",
    "                    attention_map_upsampled = F.interpolate(\n",
    "                        attention_map,\n",
    "                        size=gt_mask.shape[2:],  # This will be [H, W]\n",
    "                        mode='bilinear',\n",
    "                        align_corners=False\n",
    "                    )\n",
    "                    attention_loss += attention_criterion(attention_map_upsampled, gt_mask)\n",
    "\n",
    "                # Total loss\n",
    "                total_batch_loss = total_batch_loss + lambda_attention * attention_loss\n",
    "\n",
    "                total_loss += total_batch_loss.item()\n",
    "                val_losses.append(total_batch_loss.item())\n",
    "                \n",
    "                # Accumulate per-condition losses\n",
    "                for condition in per_condition_loss:\n",
    "                    if condition in per_condition_losses:\n",
    "                        per_condition_losses[condition] += per_condition_loss[condition]\n",
    "                \n",
    "                # Log batch validation loss to TensorBoard every 100 batches\n",
    "                if (batch_idx + 1) % 100 == 0:\n",
    "                    writer.add_scalar(f'Fold{fold}/Val_Batch_Loss', total_batch_loss.item(), epoch * len(val_loader) + batch_idx)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing batch during validation: {e}\")\n",
    "                traceback.print_exc()\n",
    "                continue  # Skip this batch\n",
    "\n",
    "    avg_val_loss = total_loss / len(val_loader) if len(val_loader) > 0 else float('inf')\n",
    "    writer.add_scalar(f'Fold{fold}/Val_Avg_Loss', avg_val_loss, epoch)\n",
    "    \n",
    "    # Log per-condition losses\n",
    "    for condition, loss in per_condition_losses.items():\n",
    "        avg_condition_loss = loss / len(val_loader)\n",
    "        writer.add_scalar(f'Fold{fold}/Val_{condition}_Loss', avg_condition_loss, epoch)\n",
    "    \n",
    "    return avg_val_loss\n",
    "\n",
    "def train_k_fold_with_custom_loss(\n",
    "    train_dataset, \n",
    "    k_folds=5, \n",
    "    num_epochs=10, \n",
    "    any_severe_scalar=0.5, \n",
    "    lambda_attention=0.1,\n",
    "    log_dir='./runs/spine_model_experiment'\n",
    "):\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "    \n",
    "    kfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "    \n",
    "    for fold, (train_ids, val_ids) in enumerate(kfold.split(train_dataset)):\n",
    "        print(f'\\nFold {fold+1}/{k_folds}')\n",
    "        \n",
    "        # Create data loaders for this fold with custom collate function\n",
    "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "        val_subsampler = torch.utils.data.SubsetRandomSampler(val_ids)\n",
    "        \n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=1,\n",
    "            sampler=train_subsampler,\n",
    "            num_workers=8,\n",
    "            pin_memory=True,\n",
    "            collate_fn=custom_collate_fn\n",
    "        )\n",
    "        \n",
    "        val_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=1,\n",
    "            sampler=val_subsampler,\n",
    "            num_workers=8,\n",
    "            pin_memory=True,\n",
    "            collate_fn=custom_collate_fn\n",
    "        )\n",
    "        \n",
    "        # Initialize the model and move it to the correct device\n",
    "        model = MultiSeriesSpineModel(num_conditions=len(train_dataset.label_columns), num_classes=3)\n",
    "        model = model.to(device)\n",
    "        \n",
    "        # Initialize optimizer, scheduler, and loss functions\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-6)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', \n",
    "                                                               factor=0.1, patience=2, verbose=True)\n",
    "        \n",
    "        # Initialize EarlyStopping with a unique path per fold\n",
    "        checkpoint_path = f'best_model_fold_{fold+1}.pth'\n",
    "        early_stopping = EarlyStopping(patience=3, path=checkpoint_path)\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
    "            \n",
    "            # Training step\n",
    "            avg_train_loss = train_one_epoch(\n",
    "                model, device, train_loader, optimizer, \n",
    "                any_severe_scalar, lambda_attention, \n",
    "                writer, fold+1, epoch+1, scheduler\n",
    "            )\n",
    "            \n",
    "            # Validation step\n",
    "            avg_val_loss = validate_one_epoch(\n",
    "                model, device, val_loader, any_severe_scalar, \n",
    "                lambda_attention, writer, fold+1, epoch+1\n",
    "            )\n",
    "            \n",
    "            print(f\"Fold {fold+1} Epoch [{epoch+1}/{num_epochs}] Avg Train Loss: {avg_train_loss:.4f}, Avg Val Loss: {avg_val_loss:.4f}\")\n",
    "        \n",
    "            # Early stopping based on validation loss\n",
    "            early_stopping(avg_val_loss, model)\n",
    "            if early_stopping.early_stop:\n",
    "                print(f\"Early stopping triggered for Fold {fold+1}!\")\n",
    "                break\n",
    "        \n",
    "        # Load the best model for this fold\n",
    "        model.load_state_dict(torch.load(checkpoint_path))\n",
    "        \n",
    "        # Save the best model for this fold\n",
    "        torch.save(model.state_dict(), f'best_model_fold_{fold+1}.pth')\n",
    "        print(f\"Best model for Fold {fold+1} saved to best_model_fold_{fold+1}.pth.\")\n",
    "    \n",
    "    # Close the TensorBoard writer after training\n",
    "    writer.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aeaf3b20-433e-4e81-a184-94af3cf553e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c72c3d7a-a50e-425b-83bd-64442a1e480e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "Navigate to the following URL:\n",
      "https://us-east-2.console.aws.amazon.com/sagemaker/home?region=us-east-2#/tensor-board-landing\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.interactive_apps import tensorboard\n",
    "\n",
    "region = \"us-east-2\"\n",
    "app = tensorboard.TensorBoardApp(region)\n",
    "print(\"Navigate to the following URL:\")\n",
    "print(\n",
    "    app.get_app_url(\n",
    "        #training_job_name=\"kaggle-rsna\", # Optional. Specify the name of the job to track.\n",
    "        open_in_default_web_browser=False           # Set to False to print the URL to terminal.\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b316883f-d2c8-418d-aac1-a32000f56967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "/opt/conda/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/opt/conda/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fold 1 Epoch 1/Training:  22%|██▏       | 289/1316 [06:09<09:21,  1.83batch/s]  "
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "n_folds = 3\n",
    "n_epochs = 10\n",
    "train_k_fold_with_custom_loss(train_dataset, k_folds=n_folds, num_epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94613ed0-a180-4284-8c66-c9a85780c610",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a602b6b-2f47-4664-88d3-c13c0ff1a80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class EnsembleModel(nn.Module):\n",
    "    def __init__(self, model_class, model_paths, device):\n",
    "        super(EnsembleModel, self).__init__()\n",
    "        self.models = nn.ModuleList()\n",
    "        for path in model_paths:\n",
    "            model = model_class()\n",
    "            model.load_state_dict(torch.load(path, map_location=device))\n",
    "            model.to(device)\n",
    "            model.eval()  # Set model to evaluation mode\n",
    "            self.models.append(model)\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, sagittal_t1, sagittal_t2_stir, axial_t2):\n",
    "        outputs_list = []\n",
    "        for model in self.models:\n",
    "            outputs, _ = model(sagittal_t1, sagittal_t2_stir, axial_t2)\n",
    "            outputs_list.append(outputs)\n",
    "        # Stack outputs and take mean over the ensemble dimension\n",
    "        outputs = torch.stack(outputs_list, dim=0)\n",
    "        avg_outputs = torch.mean(outputs, dim=0)\n",
    "        return avg_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8a504f-4591-449e-8906-5dda904b962e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "k_folds = n_folds\n",
    "model_paths = [f'model_fold_{i+1}.pth' for i in range(k_folds)]\n",
    "\n",
    "ensemble_model = EnsembleModel(\n",
    "    model_class=lambda: MultiSeriesSpineModel(num_conditions=len(LABELS), num_classes=3),\n",
    "    model_paths=model_paths,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "ensemble_model.to(device)\n",
    "ensemble_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9714202-3dcf-4676-b2bd-e937e22e2ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(ensemble_model.state_dict(), f'ensemble_model_F{n_folds}_E{n_epochs}.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89be133-6772-48b9-897f-67dabbd47e4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1650c772-19a2-427e-bdf6-2ca1e3c888ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a828ad2f-bdde-45f5-a689-5d4e46fc07e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d9e19f-db71-4cf4-a216-0590452a20f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5726bbe-2529-4bad-86ac-222faa8b0372",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dad266a-dbad-40d4-9a5b-f0a6118818e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
