{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fa7931f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T03:50:51.320210Z",
     "iopub.status.busy": "2024-10-03T03:50:51.319761Z",
     "iopub.status.idle": "2024-10-03T03:52:37.614345Z",
     "shell.execute_reply": "2024-10-03T03:52:37.613137Z"
    },
    "papermill": {
     "duration": 106.304011,
     "end_time": "2024-10-03T03:52:37.617114",
     "exception": false,
     "start_time": "2024-10-03T03:50:51.313103",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/pydicom/data/data_manager.py:375: UserWarning: A download failure occurred while attempting to retrieve US1_J2KR.dcm\n",
      "  warn_and_log(\n",
      "/opt/conda/lib/python3.10/site-packages/pydicom/data/data_manager.py:375: UserWarning: A download failure occurred while attempting to retrieve MR-SIEMENS-DICOM-WithOverlays.dcm\n",
      "  warn_and_log(\n",
      "/opt/conda/lib/python3.10/site-packages/pydicom/data/data_manager.py:375: UserWarning: A download failure occurred while attempting to retrieve OBXXXX1A.dcm\n",
      "  warn_and_log(\n",
      "/opt/conda/lib/python3.10/site-packages/pydicom/data/data_manager.py:375: UserWarning: A download failure occurred while attempting to retrieve US1_UNCR.dcm\n",
      "  warn_and_log(\n",
      "/opt/conda/lib/python3.10/site-packages/pydicom/data/data_manager.py:375: UserWarning: A download failure occurred while attempting to retrieve color3d_jpeg_baseline.dcm\n",
      "  warn_and_log(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pydicom\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10569b81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T03:52:37.628718Z",
     "iopub.status.busy": "2024-10-03T03:52:37.628090Z",
     "iopub.status.idle": "2024-10-03T03:52:37.634102Z",
     "shell.execute_reply": "2024-10-03T03:52:37.633075Z"
    },
    "papermill": {
     "duration": 0.014171,
     "end_time": "2024-10-03T03:52:37.636303",
     "exception": false,
     "start_time": "2024-10-03T03:52:37.622132",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define constants\n",
    "SERIES_DESCRIPTIONS = ['Sagittal T1', 'Sagittal T2_STIR', 'Axial T2']\n",
    "IMG_SIZE = (512, 512)\n",
    "TARGET_SLICES = 10\n",
    "rd = '/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0f0eab7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T03:52:37.647564Z",
     "iopub.status.busy": "2024-10-03T03:52:37.647039Z",
     "iopub.status.idle": "2024-10-03T03:52:37.668557Z",
     "shell.execute_reply": "2024-10-03T03:52:37.667487Z"
    },
    "papermill": {
     "duration": 0.030139,
     "end_time": "2024-10-03T03:52:37.670887",
     "exception": false,
     "start_time": "2024-10-03T03:52:37.640748",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helper functions for natural sorting\n",
    "def atoi(text):\n",
    "    return int(text) if text.isdigit() else text\n",
    "\n",
    "def natural_keys(text):\n",
    "    \"\"\"\n",
    "    Alphanumeric sorting helper function.\n",
    "    \"\"\"\n",
    "    return [atoi(c) for c in re.split(r'(\\d+)', text)]\n",
    "\n",
    "# Function to resample slices\n",
    "def resample_slices(volume, target_slices=10):\n",
    "    \"\"\"\n",
    "    Resample or pad the number of slices to target_slices.\n",
    "    Args:\n",
    "        volume (np.ndarray): 3D array of shape [slices, H, W].\n",
    "        target_slices (int): Desired number of slices.\n",
    "    Returns:\n",
    "        np.ndarray: Resampled 3D array.\n",
    "    \"\"\"\n",
    "    current_slices = volume.shape[0]\n",
    "    if current_slices == target_slices:\n",
    "        return volume\n",
    "    elif current_slices > target_slices:\n",
    "        indices = np.linspace(0, current_slices - 1, target_slices).astype(int)\n",
    "        return volume[indices]\n",
    "    else:\n",
    "        # Pad with zeros\n",
    "        pad_width = target_slices - current_slices\n",
    "        padding = ((0, pad_width), (0, 0), (0, 0))\n",
    "        return np.pad(volume, padding, mode='constant', constant_values=0)\n",
    "\n",
    "# Define preprocessing transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Converts PIL Image to Tensor\n",
    "    # Note: We will apply Normalize3D after stacking the scans\n",
    "])\n",
    "\n",
    "# Define the test dataset class\n",
    "# Define the test dataset class\n",
    "class LumbarSpineTestDataset(Dataset):\n",
    "    def __init__(self, df, study_ids, transform=None, target_slices=10):\n",
    "        self.df = df\n",
    "        self.study_ids = study_ids\n",
    "        self.transform = transform\n",
    "        self.target_slices = target_slices\n",
    "\n",
    "    def get_img_paths(self, study_id, series_description):\n",
    "        pdf = self.df[self.df['study_id'] == study_id]\n",
    "        pdf_series = pdf[pdf['series_description'] == series_description]\n",
    "        image_paths = []\n",
    "        for idx, row in pdf_series.iterrows():\n",
    "            series_id = row['series_id']\n",
    "            paths = glob.glob(f'{rd}/test_images/{study_id}/{series_id}/*.dcm')\n",
    "            paths = sorted(paths, key=natural_keys)\n",
    "            image_paths.extend(paths)\n",
    "        return image_paths\n",
    "\n",
    "    def read_dcm_image(self, path):\n",
    "        dicom_data = pydicom.dcmread(path)\n",
    "        image = dicom_data.pixel_array.astype(np.float32)\n",
    "        # Normalize the image to [0, 255]\n",
    "        image = (image - image.min()) / (image.max() - image.min() + 1e-6) * 255.0\n",
    "        # Resize image to 512x512 using the same interpolation as training\n",
    "        image = cv2.resize(image, (512, 512), interpolation=cv2.INTER_CUBIC)\n",
    "        # Convert to PIL Image in grayscale\n",
    "        image = Image.fromarray(image.astype(np.uint8)).convert('L')\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.study_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        study_id = self.study_ids[idx]\n",
    "        scans = []\n",
    "        for series_description in SERIES_DESCRIPTIONS:\n",
    "            image_paths = self.get_img_paths(study_id, series_description)\n",
    "            if not image_paths:\n",
    "                # If no images, create a zero tensor\n",
    "                scan = torch.zeros((self.target_slices, IMG_SIZE[0], IMG_SIZE[1]))\n",
    "                scans.append(scan)\n",
    "                continue\n",
    "            # Read images\n",
    "            series_images = []\n",
    "            for img_path in image_paths:\n",
    "                try:\n",
    "                    img = self.read_dcm_image(img_path)\n",
    "                    img = transform(img)  # Apply transform\n",
    "                    img = img.squeeze(0)  # Remove channel dimension\n",
    "                    series_images.append(img)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading image {img_path}: {e}\")\n",
    "            if series_images:\n",
    "                series_tensor = torch.stack(series_images, dim=0)  # Shape: [num_slices, H, W]\n",
    "            else:\n",
    "                series_tensor = torch.zeros((1, IMG_SIZE[0], IMG_SIZE[1]))\n",
    "            # Resample slices to TARGET_SLICES\n",
    "            series_tensor = resample_slices(series_tensor.numpy(), target_slices=self.target_slices)\n",
    "            series_tensor = torch.from_numpy(series_tensor)\n",
    "            scans.append(series_tensor)\n",
    "        # Stack scans along the channel dimension\n",
    "        scan = torch.stack(scans, dim=0)  # Shape: [3, TARGET_SLICES, H, W]\n",
    "        # Apply Normalize3D after stacking\n",
    "        if self.transform:\n",
    "            scan = self.transform(scan)  # Apply any additional transforms\n",
    "        sample = {\n",
    "            'study_id': study_id,\n",
    "            'scan': scan\n",
    "        }\n",
    "        return sample\n",
    "    \n",
    "class Normalize3D(object):\n",
    "    def __init__(self, mean, std):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            mean (list or tuple): Mean values for each channel.\n",
    "            std (list or tuple): Standard deviation values for each channel.\n",
    "        \"\"\"\n",
    "        self.mean = torch.tensor(mean).view(-1, 1, 1, 1)  # Shape: [C, 1, 1, 1]\n",
    "        self.std = torch.tensor(std).view(-1, 1, 1, 1)    # Shape: [C, 1, 1, 1]\n",
    "    \n",
    "    def __call__(self, tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensor (torch.Tensor): Tensor image of size [C, D, H, W] to be normalized.\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Normalized tensor.\n",
    "        \"\"\"\n",
    "        return (tensor - self.mean) / self.std\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d0e5e57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T03:52:37.681470Z",
     "iopub.status.busy": "2024-10-03T03:52:37.681035Z",
     "iopub.status.idle": "2024-10-03T03:52:37.730142Z",
     "shell.execute_reply": "2024-10-03T03:52:37.729046Z"
    },
    "papermill": {
     "duration": 0.057423,
     "end_time": "2024-10-03T03:52:37.732753",
     "exception": false,
     "start_time": "2024-10-03T03:52:37.675330",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read test_series_descriptions.csv\n",
    "test_df = pd.read_csv(f'{rd}/test_series_descriptions.csv')\n",
    "\n",
    "# Replace 'T2/STIR' with 'T2_STIR' in series descriptions\n",
    "test_df['series_description'] = test_df['series_description'].str.replace('T2/STIR', 'T2_STIR')\n",
    "\n",
    "study_ids = test_df['study_id'].unique()\n",
    "\n",
    "# Create the test dataset and dataloader\n",
    "test_dataset = LumbarSpineTestDataset(\n",
    "    df=test_df,\n",
    "    study_ids=study_ids,\n",
    "    transform=transforms.Compose([\n",
    "        Normalize3D(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Apply Normalize3D\n",
    "    ]),\n",
    "    target_slices=TARGET_SLICES\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=0,  # Adjust based on your system\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# Define your label names consistent with training\n",
    "CONDITIONS = [\n",
    "    'spinal_canal_stenosis', \n",
    "    'left_neural_foraminal_narrowing', \n",
    "    'right_neural_foraminal_narrowing',\n",
    "    'left_subarticular_stenosis',\n",
    "    'right_subarticular_stenosis'\n",
    "]\n",
    "\n",
    "LEVELS = [\n",
    "    'l1_l2',\n",
    "    'l2_l3',\n",
    "    'l3_l4',\n",
    "    'l4_l5',\n",
    "    'l5_s1',\n",
    "]\n",
    "\n",
    "LABELS = [f'{condition}_{level}' for condition in CONDITIONS for level in LEVELS]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "777fd400",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T03:52:37.743390Z",
     "iopub.status.busy": "2024-10-03T03:52:37.742961Z",
     "iopub.status.idle": "2024-10-03T03:52:37.768498Z",
     "shell.execute_reply": "2024-10-03T03:52:37.767372Z"
    },
    "papermill": {
     "duration": 0.033757,
     "end_time": "2024-10-03T03:52:37.770924",
     "exception": false,
     "start_time": "2024-10-03T03:52:37.737167",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool3d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // reduction, channel),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "class BasicBlock3D(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock3D, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(in_planes, planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm3d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv3d(planes, planes, kernel_size=3, stride=1,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm3d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "        self.se = SEBlock(planes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        out = self.se(out)  # Apply SE block\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class ResNet3D(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=512):\n",
    "        super(ResNet3D, self).__init__()\n",
    "        self.in_planes = 64\n",
    "        # Updated to accept 3 channels instead of 1\n",
    "        self.conv1 = nn.Conv3d(3, 64, kernel_size=7, stride=(1, 2, 2), padding=(3, 3, 3), bias=False)\n",
    "        self.bn1 = nn.BatchNorm3d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool3d(kernel_size=3, stride=(1, 2, 2), padding=1)\n",
    "\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool3d((1, 1, 1))\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_planes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv3d(self.in_planes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm3d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.in_planes, planes, stride, downsample))\n",
    "        self.in_planes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.in_planes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input x: [batch_size, 3, slices, H, W]\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)  # [batch, 64, slices, H/4, W/4]\n",
    "        x = self.layer2(x)  # [batch, 128, slices/2, H/8, W/8]\n",
    "        x = self.layer3(x)  # [batch, 256, slices/4, H/16, W/16]\n",
    "        x = self.layer4(x)  # [batch, 512, slices/8, H/32, W/32]\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "        return x  # [batch_size, num_classes]\n",
    "\n",
    "class CoordAttentionModule(nn.Module):\n",
    "    def __init__(self, feature_dim, coord_dim):\n",
    "        super(CoordAttentionModule, self).__init__()\n",
    "        self.attention_fc = nn.Sequential(\n",
    "            nn.Linear(coord_dim, feature_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(feature_dim, feature_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, coords):\n",
    "        attention_weights = self.attention_fc(coords)  # [batch_size, feature_dim]\n",
    "        x = x * attention_weights  # Element-wise multiplication\n",
    "        return x\n",
    "\n",
    "class CoordAttention3DResNet(nn.Module):\n",
    "    def __init__(self, num_classes, coord_dim):\n",
    "        super(CoordAttention3DResNet, self).__init__()\n",
    "        self.resnet3d = ResNet3D(BasicBlock3D, [2, 2, 2, 2], num_classes=512)\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "        self.coord_attention = CoordAttentionModule(512, coord_dim)\n",
    "\n",
    "    def forward(self, x, coords=None):\n",
    "        x = self.resnet3d(x)  # [batch_size, 512]\n",
    "        if self.training and coords is not None:\n",
    "            x = self.coord_attention(x, coords)\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "568e3f4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T03:52:37.781493Z",
     "iopub.status.busy": "2024-10-03T03:52:37.781056Z",
     "iopub.status.idle": "2024-10-03T03:52:37.788818Z",
     "shell.execute_reply": "2024-10-03T03:52:37.787740Z"
    },
    "papermill": {
     "duration": 0.016472,
     "end_time": "2024-10-03T03:52:37.791898",
     "exception": false,
     "start_time": "2024-10-03T03:52:37.775426",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now, define the EnsembleModel class\n",
    "class EnsembleModel(nn.Module):\n",
    "    def __init__(self, model_class, num_models, device):\n",
    "        super(EnsembleModel, self).__init__()\n",
    "        self.models = nn.ModuleList()\n",
    "        for _ in range(num_models):\n",
    "            # Initialize the model with necessary parameters\n",
    "            model = model_class()\n",
    "            model.to(device)\n",
    "            model.eval()  # Set model to evaluation mode\n",
    "            self.models.append(model)\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs_list = []\n",
    "        for model in self.models:\n",
    "            outputs = model(x)\n",
    "            outputs_list.append(outputs)\n",
    "        # Stack outputs and take mean over the ensemble dimension\n",
    "        outputs = torch.stack(outputs_list, dim=0)\n",
    "        avg_outputs = torch.mean(outputs, dim=0)\n",
    "        return avg_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebda2049",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T03:52:37.802630Z",
     "iopub.status.busy": "2024-10-03T03:52:37.802181Z",
     "iopub.status.idle": "2024-10-03T03:52:40.970042Z",
     "shell.execute_reply": "2024-10-03T03:52:40.968942Z"
    },
    "papermill": {
     "duration": 3.175973,
     "end_time": "2024-10-03T03:52:40.972456",
     "exception": false,
     "start_time": "2024-10-03T03:52:37.796483",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17/4191411991.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ensemble_model.load_state_dict(torch.load(ensemble_model_path, map_location=device))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EnsembleModel(\n",
       "  (models): ModuleList(\n",
       "    (0-1): 2 x CoordAttention3DResNet(\n",
       "      (resnet3d): ResNet3D(\n",
       "        (conv1): Conv3d(3, 64, kernel_size=(7, 7, 7), stride=(1, 2, 2), padding=(3, 3, 3), bias=False)\n",
       "        (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (maxpool): MaxPool3d(kernel_size=3, stride=(1, 2, 2), padding=1, dilation=1, ceil_mode=False)\n",
       "        (layer1): Sequential(\n",
       "          (0): BasicBlock3D(\n",
       "            (conv1): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "            (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (conv2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "            (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (se): SEBlock(\n",
       "              (avg_pool): AdaptiveAvgPool3d(output_size=1)\n",
       "              (fc): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=4, bias=True)\n",
       "                (1): ReLU(inplace=True)\n",
       "                (2): Linear(in_features=4, out_features=64, bias=True)\n",
       "                (3): Sigmoid()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1): BasicBlock3D(\n",
       "            (conv1): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "            (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (conv2): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "            (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (se): SEBlock(\n",
       "              (avg_pool): AdaptiveAvgPool3d(output_size=1)\n",
       "              (fc): Sequential(\n",
       "                (0): Linear(in_features=64, out_features=4, bias=True)\n",
       "                (1): ReLU(inplace=True)\n",
       "                (2): Linear(in_features=4, out_features=64, bias=True)\n",
       "                (3): Sigmoid()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (layer2): Sequential(\n",
       "          (0): BasicBlock3D(\n",
       "            (conv1): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "            (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (conv2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "            (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (downsample): Sequential(\n",
       "              (0): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
       "              (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (se): SEBlock(\n",
       "              (avg_pool): AdaptiveAvgPool3d(output_size=1)\n",
       "              (fc): Sequential(\n",
       "                (0): Linear(in_features=128, out_features=8, bias=True)\n",
       "                (1): ReLU(inplace=True)\n",
       "                (2): Linear(in_features=8, out_features=128, bias=True)\n",
       "                (3): Sigmoid()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1): BasicBlock3D(\n",
       "            (conv1): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "            (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (conv2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "            (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (se): SEBlock(\n",
       "              (avg_pool): AdaptiveAvgPool3d(output_size=1)\n",
       "              (fc): Sequential(\n",
       "                (0): Linear(in_features=128, out_features=8, bias=True)\n",
       "                (1): ReLU(inplace=True)\n",
       "                (2): Linear(in_features=8, out_features=128, bias=True)\n",
       "                (3): Sigmoid()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (layer3): Sequential(\n",
       "          (0): BasicBlock3D(\n",
       "            (conv1): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "            (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "            (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (downsample): Sequential(\n",
       "              (0): Conv3d(128, 256, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
       "              (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (se): SEBlock(\n",
       "              (avg_pool): AdaptiveAvgPool3d(output_size=1)\n",
       "              (fc): Sequential(\n",
       "                (0): Linear(in_features=256, out_features=16, bias=True)\n",
       "                (1): ReLU(inplace=True)\n",
       "                (2): Linear(in_features=16, out_features=256, bias=True)\n",
       "                (3): Sigmoid()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1): BasicBlock3D(\n",
       "            (conv1): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "            (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "            (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (se): SEBlock(\n",
       "              (avg_pool): AdaptiveAvgPool3d(output_size=1)\n",
       "              (fc): Sequential(\n",
       "                (0): Linear(in_features=256, out_features=16, bias=True)\n",
       "                (1): ReLU(inplace=True)\n",
       "                (2): Linear(in_features=16, out_features=256, bias=True)\n",
       "                (3): Sigmoid()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (layer4): Sequential(\n",
       "          (0): BasicBlock3D(\n",
       "            (conv1): Conv3d(256, 512, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "            (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "            (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (downsample): Sequential(\n",
       "              (0): Conv3d(256, 512, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
       "              (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (se): SEBlock(\n",
       "              (avg_pool): AdaptiveAvgPool3d(output_size=1)\n",
       "              (fc): Sequential(\n",
       "                (0): Linear(in_features=512, out_features=32, bias=True)\n",
       "                (1): ReLU(inplace=True)\n",
       "                (2): Linear(in_features=32, out_features=512, bias=True)\n",
       "                (3): Sigmoid()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1): BasicBlock3D(\n",
       "            (conv1): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "            (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "            (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (se): SEBlock(\n",
       "              (avg_pool): AdaptiveAvgPool3d(output_size=1)\n",
       "              (fc): Sequential(\n",
       "                (0): Linear(in_features=512, out_features=32, bias=True)\n",
       "                (1): ReLU(inplace=True)\n",
       "                (2): Linear(in_features=32, out_features=512, bias=True)\n",
       "                (3): Sigmoid()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (avgpool): AdaptiveAvgPool3d(output_size=(1, 1, 1))\n",
       "        (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "        (fc): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (fc): Linear(in_features=512, out_features=75, bias=True)\n",
       "      (coord_attention): CoordAttentionModule(\n",
       "        (attention_fc): Sequential(\n",
       "          (0): Linear(in_features=50, out_features=512, bias=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (3): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the ensemble model\n",
    "num_conditions = 25  # Number of labels\n",
    "num_classes_per_label = 3  # Number of classes per label\n",
    "total_num_classes = num_conditions * num_classes_per_label\n",
    "coord_dim = len(CONDITIONS) * len(LEVELS) * 2  # As per your training setup\n",
    "\n",
    "# Set the number of folds and epochs (adjust according to your saved model)\n",
    "k_folds = 2  # Number of models in the ensemble\n",
    "num_epochs = 5  # Number of epochs used during training\n",
    "\n",
    "# Define the model class used in the ensemble\n",
    "def model_class():\n",
    "    return CoordAttention3DResNet(num_classes=total_num_classes, coord_dim=coord_dim)\n",
    "\n",
    "# Instantiate the ensemble model\n",
    "ensemble_model = EnsembleModel(\n",
    "    model_class=model_class,\n",
    "    num_models=k_folds,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Load the ensembled model weights\n",
    "ensemble_model_path = '/kaggle/input/rsna-chacha-pytorch-models/pytorch/default/17/ensemble_3d_resnet_model_F2_E5.pth'\n",
    "ensemble_model.load_state_dict(torch.load(ensemble_model_path, map_location=device))\n",
    "ensemble_model.to(device)\n",
    "ensemble_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42853a95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T03:52:40.983724Z",
     "iopub.status.busy": "2024-10-03T03:52:40.983288Z",
     "iopub.status.idle": "2024-10-03T03:52:50.402301Z",
     "shell.execute_reply": "2024-10-03T03:52:50.401150Z"
    },
    "papermill": {
     "duration": 9.427134,
     "end_time": "2024-10-03T03:52:50.404537",
     "exception": false,
     "start_time": "2024-10-03T03:52:40.977403",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:09<00:00,  9.41s/it]\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists for row names and predictions\n",
    "row_names = []\n",
    "predictions = []\n",
    "num_classes = 3\n",
    "\n",
    "# Perform predictions\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader):\n",
    "        study_id = batch['study_id'][0]\n",
    "        scan = batch['scan']  # Shape: [1, 3, slices, H, W]\n",
    "        scan = scan.to(device)\n",
    "        outputs = ensemble_model(scan)  # Shape: [batch_size, total_num_classes]\n",
    "        outputs = outputs.squeeze(0)  # Shape: [total_num_classes]\n",
    "        outputs = outputs.view(num_conditions, num_classes)  # Shape: [num_conditions, num_classes]\n",
    "\n",
    "        # Apply softmax to get probabilities\n",
    "        probs = F.softmax(outputs, dim=1)  # Shape: [num_conditions, num_classes]\n",
    "\n",
    "        # Ensure the predictions are aligned with LABELS\n",
    "        pred_per_study = probs.cpu().numpy()  # Shape: [num_conditions, num_classes]\n",
    "\n",
    "        # Generate row names and collect predictions\n",
    "        for label in LABELS:\n",
    "            row_names.append(f'{study_id}_{label}')\n",
    "        predictions.append(pred_per_study)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a0a78a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T03:52:50.416670Z",
     "iopub.status.busy": "2024-10-03T03:52:50.416263Z",
     "iopub.status.idle": "2024-10-03T03:52:50.428861Z",
     "shell.execute_reply": "2024-10-03T03:52:50.427498Z"
    },
    "papermill": {
     "duration": 0.021782,
     "end_time": "2024-10-03T03:52:50.431471",
     "exception": false,
     "start_time": "2024-10-03T03:52:50.409689",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file 'submission.csv' has been generated.\n"
     ]
    }
   ],
   "source": [
    "# Stack predictions\n",
    "predictions = np.vstack(predictions)  # Shape: [num_studies * num_conditions, num_classes]\n",
    "\n",
    "# Create the submission DataFrame\n",
    "submission = pd.DataFrame({\n",
    "    'row_id': row_names,\n",
    "    'normal_mild': predictions[:, 0],\n",
    "    'moderate': predictions[:, 1],\n",
    "    'severe': predictions[:, 2]\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"Submission file 'submission.csv' has been generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47daa42f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T03:52:50.444390Z",
     "iopub.status.busy": "2024-10-03T03:52:50.443855Z",
     "iopub.status.idle": "2024-10-03T03:52:50.462514Z",
     "shell.execute_reply": "2024-10-03T03:52:50.461383Z"
    },
    "papermill": {
     "duration": 0.028163,
     "end_time": "2024-10-03T03:52:50.464971",
     "exception": false,
     "start_time": "2024-10-03T03:52:50.436808",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>normal_mild</th>\n",
       "      <th>moderate</th>\n",
       "      <th>severe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44036939_spinal_canal_stenosis_l1_l2</td>\n",
       "      <td>0.940144</td>\n",
       "      <td>0.037570</td>\n",
       "      <td>0.022285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44036939_spinal_canal_stenosis_l2_l3</td>\n",
       "      <td>0.834173</td>\n",
       "      <td>0.115013</td>\n",
       "      <td>0.050813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44036939_spinal_canal_stenosis_l3_l4</td>\n",
       "      <td>0.658272</td>\n",
       "      <td>0.216996</td>\n",
       "      <td>0.124732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44036939_spinal_canal_stenosis_l4_l5</td>\n",
       "      <td>0.620966</td>\n",
       "      <td>0.192703</td>\n",
       "      <td>0.186331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>44036939_spinal_canal_stenosis_l5_s1</td>\n",
       "      <td>0.934442</td>\n",
       "      <td>0.042500</td>\n",
       "      <td>0.023058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>44036939_left_neural_foraminal_narrowing_l1_l2</td>\n",
       "      <td>0.949889</td>\n",
       "      <td>0.042989</td>\n",
       "      <td>0.007123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>44036939_left_neural_foraminal_narrowing_l2_l3</td>\n",
       "      <td>0.875555</td>\n",
       "      <td>0.109058</td>\n",
       "      <td>0.015388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>44036939_left_neural_foraminal_narrowing_l3_l4</td>\n",
       "      <td>0.619475</td>\n",
       "      <td>0.335656</td>\n",
       "      <td>0.044869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>44036939_left_neural_foraminal_narrowing_l4_l5</td>\n",
       "      <td>0.499927</td>\n",
       "      <td>0.428991</td>\n",
       "      <td>0.071083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>44036939_left_neural_foraminal_narrowing_l5_s1</td>\n",
       "      <td>0.445183</td>\n",
       "      <td>0.418560</td>\n",
       "      <td>0.136257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>44036939_right_neural_foraminal_narrowing_l1_l2</td>\n",
       "      <td>0.931041</td>\n",
       "      <td>0.049977</td>\n",
       "      <td>0.018982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>44036939_right_neural_foraminal_narrowing_l2_l3</td>\n",
       "      <td>0.895951</td>\n",
       "      <td>0.094341</td>\n",
       "      <td>0.009707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>44036939_right_neural_foraminal_narrowing_l3_l4</td>\n",
       "      <td>0.740919</td>\n",
       "      <td>0.230062</td>\n",
       "      <td>0.029019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>44036939_right_neural_foraminal_narrowing_l4_l5</td>\n",
       "      <td>0.578464</td>\n",
       "      <td>0.319720</td>\n",
       "      <td>0.101816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>44036939_right_neural_foraminal_narrowing_l5_s1</td>\n",
       "      <td>0.545589</td>\n",
       "      <td>0.330202</td>\n",
       "      <td>0.124209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>44036939_left_subarticular_stenosis_l1_l2</td>\n",
       "      <td>0.902098</td>\n",
       "      <td>0.061705</td>\n",
       "      <td>0.036198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>44036939_left_subarticular_stenosis_l2_l3</td>\n",
       "      <td>0.758175</td>\n",
       "      <td>0.152376</td>\n",
       "      <td>0.089449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>44036939_left_subarticular_stenosis_l3_l4</td>\n",
       "      <td>0.604342</td>\n",
       "      <td>0.237926</td>\n",
       "      <td>0.157733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>44036939_left_subarticular_stenosis_l4_l5</td>\n",
       "      <td>0.393734</td>\n",
       "      <td>0.347427</td>\n",
       "      <td>0.258839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>44036939_left_subarticular_stenosis_l5_s1</td>\n",
       "      <td>0.594406</td>\n",
       "      <td>0.280428</td>\n",
       "      <td>0.125166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>44036939_right_subarticular_stenosis_l1_l2</td>\n",
       "      <td>0.897692</td>\n",
       "      <td>0.072035</td>\n",
       "      <td>0.030273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>44036939_right_subarticular_stenosis_l2_l3</td>\n",
       "      <td>0.770484</td>\n",
       "      <td>0.157046</td>\n",
       "      <td>0.072470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>44036939_right_subarticular_stenosis_l3_l4</td>\n",
       "      <td>0.599487</td>\n",
       "      <td>0.258003</td>\n",
       "      <td>0.142510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>44036939_right_subarticular_stenosis_l4_l5</td>\n",
       "      <td>0.431440</td>\n",
       "      <td>0.304515</td>\n",
       "      <td>0.264044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>44036939_right_subarticular_stenosis_l5_s1</td>\n",
       "      <td>0.632390</td>\n",
       "      <td>0.252321</td>\n",
       "      <td>0.115289</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             row_id  normal_mild  moderate  \\\n",
       "0              44036939_spinal_canal_stenosis_l1_l2     0.940144  0.037570   \n",
       "1              44036939_spinal_canal_stenosis_l2_l3     0.834173  0.115013   \n",
       "2              44036939_spinal_canal_stenosis_l3_l4     0.658272  0.216996   \n",
       "3              44036939_spinal_canal_stenosis_l4_l5     0.620966  0.192703   \n",
       "4              44036939_spinal_canal_stenosis_l5_s1     0.934442  0.042500   \n",
       "5    44036939_left_neural_foraminal_narrowing_l1_l2     0.949889  0.042989   \n",
       "6    44036939_left_neural_foraminal_narrowing_l2_l3     0.875555  0.109058   \n",
       "7    44036939_left_neural_foraminal_narrowing_l3_l4     0.619475  0.335656   \n",
       "8    44036939_left_neural_foraminal_narrowing_l4_l5     0.499927  0.428991   \n",
       "9    44036939_left_neural_foraminal_narrowing_l5_s1     0.445183  0.418560   \n",
       "10  44036939_right_neural_foraminal_narrowing_l1_l2     0.931041  0.049977   \n",
       "11  44036939_right_neural_foraminal_narrowing_l2_l3     0.895951  0.094341   \n",
       "12  44036939_right_neural_foraminal_narrowing_l3_l4     0.740919  0.230062   \n",
       "13  44036939_right_neural_foraminal_narrowing_l4_l5     0.578464  0.319720   \n",
       "14  44036939_right_neural_foraminal_narrowing_l5_s1     0.545589  0.330202   \n",
       "15        44036939_left_subarticular_stenosis_l1_l2     0.902098  0.061705   \n",
       "16        44036939_left_subarticular_stenosis_l2_l3     0.758175  0.152376   \n",
       "17        44036939_left_subarticular_stenosis_l3_l4     0.604342  0.237926   \n",
       "18        44036939_left_subarticular_stenosis_l4_l5     0.393734  0.347427   \n",
       "19        44036939_left_subarticular_stenosis_l5_s1     0.594406  0.280428   \n",
       "20       44036939_right_subarticular_stenosis_l1_l2     0.897692  0.072035   \n",
       "21       44036939_right_subarticular_stenosis_l2_l3     0.770484  0.157046   \n",
       "22       44036939_right_subarticular_stenosis_l3_l4     0.599487  0.258003   \n",
       "23       44036939_right_subarticular_stenosis_l4_l5     0.431440  0.304515   \n",
       "24       44036939_right_subarticular_stenosis_l5_s1     0.632390  0.252321   \n",
       "\n",
       "      severe  \n",
       "0   0.022285  \n",
       "1   0.050813  \n",
       "2   0.124732  \n",
       "3   0.186331  \n",
       "4   0.023058  \n",
       "5   0.007123  \n",
       "6   0.015388  \n",
       "7   0.044869  \n",
       "8   0.071083  \n",
       "9   0.136257  \n",
       "10  0.018982  \n",
       "11  0.009707  \n",
       "12  0.029019  \n",
       "13  0.101816  \n",
       "14  0.124209  \n",
       "15  0.036198  \n",
       "16  0.089449  \n",
       "17  0.157733  \n",
       "18  0.258839  \n",
       "19  0.125166  \n",
       "20  0.030273  \n",
       "21  0.072470  \n",
       "22  0.142510  \n",
       "23  0.264044  \n",
       "24  0.115289  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db979f80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-03T03:52:50.478918Z",
     "iopub.status.busy": "2024-10-03T03:52:50.478480Z",
     "iopub.status.idle": "2024-10-03T03:52:50.484289Z",
     "shell.execute_reply": "2024-10-03T03:52:50.483085Z"
    },
    "papermill": {
     "duration": 0.015931,
     "end_time": "2024-10-03T03:52:50.486541",
     "exception": false,
     "start_time": "2024-10-03T03:52:50.470610",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10d79ec",
   "metadata": {
    "papermill": {
     "duration": 0.005387,
     "end_time": "2024-10-03T03:52:50.497748",
     "exception": false,
     "start_time": "2024-10-03T03:52:50.492361",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 8561470,
     "sourceId": 71549,
     "sourceType": "competition"
    },
    {
     "modelId": 120206,
     "modelInstanceId": 96019,
     "sourceId": 125911,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30775,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 123.048037,
   "end_time": "2024-10-03T03:52:51.726523",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-10-03T03:50:48.678486",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
